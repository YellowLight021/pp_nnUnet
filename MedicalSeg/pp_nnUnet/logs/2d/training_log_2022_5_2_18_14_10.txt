Starting... 
2022-05-02 18:14:10.478857: TRAINING KEYS:
 odict_keys(['lung_001', 'lung_003', 'lung_004', 'lung_005', 'lung_006', 'lung_009', 'lung_010', 'lung_014', 'lung_015', 'lung_016', 'lung_018', 'lung_020', 'lung_022', 'lung_023', 'lung_025', 'lung_026', 'lung_027', 'lung_028', 'lung_029', 'lung_031', 'lung_033', 'lung_034', 'lung_036', 'lung_037', 'lung_038', 'lung_041', 'lung_042', 'lung_043', 'lung_044', 'lung_045', 'lung_046', 'lung_047', 'lung_048', 'lung_049', 'lung_051', 'lung_053', 'lung_054', 'lung_055', 'lung_057', 'lung_058', 'lung_059', 'lung_061', 'lung_062', 'lung_064', 'lung_065', 'lung_066', 'lung_069', 'lung_070', 'lung_071', 'lung_073', 'lung_074', 'lung_075', 'lung_078', 'lung_079', 'lung_080', 'lung_081', 'lung_083', 'lung_084', 'lung_086', 'lung_092', 'lung_093', 'lung_095', 'lung_096']) 
2022-05-02 18:14:10.479418: VALIDATION KEYS:
 odict_keys(['lung_001', 'lung_003', 'lung_004', 'lung_005', 'lung_006', 'lung_009', 'lung_010', 'lung_014', 'lung_015', 'lung_016', 'lung_018', 'lung_020', 'lung_022', 'lung_023', 'lung_025', 'lung_026', 'lung_027', 'lung_028', 'lung_029', 'lung_031', 'lung_033', 'lung_034', 'lung_036', 'lung_037', 'lung_038', 'lung_041', 'lung_042', 'lung_043', 'lung_044', 'lung_045', 'lung_046', 'lung_047', 'lung_048', 'lung_049', 'lung_051', 'lung_053', 'lung_054', 'lung_055', 'lung_057', 'lung_058', 'lung_059', 'lung_061', 'lung_062', 'lung_064', 'lung_065', 'lung_066', 'lung_069', 'lung_070', 'lung_071', 'lung_073', 'lung_074', 'lung_075', 'lung_078', 'lung_079', 'lung_080', 'lung_081', 'lung_083', 'lung_084', 'lung_086', 'lung_092', 'lung_093', 'lung_095', 'lung_096']) 
2022-05-02 18:14:15.026598: loading checkpoint /home/aistudio/Dataset/nnUnet_trained_models/nnUNet/2d/Task006_Lung/nnUNetTrainerV2__nnUNetPlansv2.1/all/model_latest.model train= True 
2022-05-02 18:14:15.463668: lr: 0.003238 
2022-05-02 18:14:17.327103: Unable to plot network architecture: 
2022-05-02 18:14:17.327353: No module named 'hiddenlayer' 
2022-05-02 18:14:17.327428: 
printing the network instead:
 
2022-05-02 18:14:17.327474: Generic_UNet(
  (conv_blocks_localization): LayerList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(960, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(480, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(960, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(480, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(960, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(480, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(512, 256, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(256, 256, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(256, 128, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(128, 128, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(128, 64, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(64, 64, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (6): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(64, 32, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(32, 32, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (conv_blocks_context): LayerList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2D(1, 32, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2D(32, 32, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2D(32, 64, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2D(64, 64, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2D(64, 128, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2D(128, 128, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2D(128, 256, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2D(256, 256, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2D(256, 480, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2D(480, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (5): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2D(480, 480, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2D(480, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (6): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2D(480, 480, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2D(480, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (7): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(480, 480, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(480, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (td): LayerList()
  (tu): LayerList(
    (0): Conv2DTranspose(480, 480, kernel_size=[2, 2], stride=[2, 2], data_format=NCHW)
    (1): Conv2DTranspose(480, 480, kernel_size=[2, 2], stride=[2, 2], data_format=NCHW)
    (2): Conv2DTranspose(480, 480, kernel_size=[2, 2], stride=[2, 2], data_format=NCHW)
    (3): Conv2DTranspose(480, 256, kernel_size=[2, 2], stride=[2, 2], data_format=NCHW)
    (4): Conv2DTranspose(256, 128, kernel_size=[2, 2], stride=[2, 2], data_format=NCHW)
    (5): Conv2DTranspose(128, 64, kernel_size=[2, 2], stride=[2, 2], data_format=NCHW)
    (6): Conv2DTranspose(64, 32, kernel_size=[2, 2], stride=[2, 2], data_format=NCHW)
  )
  (seg_outputs): LayerList(
    (0): Conv2D(480, 2, kernel_size=[1, 1], data_format=NCHW)
    (1): Conv2D(480, 2, kernel_size=[1, 1], data_format=NCHW)
    (2): Conv2D(480, 2, kernel_size=[1, 1], data_format=NCHW)
    (3): Conv2D(256, 2, kernel_size=[1, 1], data_format=NCHW)
    (4): Conv2D(128, 2, kernel_size=[1, 1], data_format=NCHW)
    (5): Conv2D(64, 2, kernel_size=[1, 1], data_format=NCHW)
    (6): Conv2D(32, 2, kernel_size=[1, 1], data_format=NCHW)
  )
) 
2022-05-02 18:14:17.330911: 
 
2022-05-02 18:14:17.331075: 
epoch:  50 
2022-05-02 18:21:58.886543: train loss : -0.7265 
2022-05-02 18:22:31.687649: validation loss: -0.7511 
2022-05-02 18:22:31.688428: Average global foreground Dice: [0.7916] 
2022-05-02 18:22:31.688673: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 18:22:32.685375: lr: 0.003092 
2022-05-02 18:22:32.857996: saving checkpoint... 
2022-05-02 18:22:33.431273: done, saving took 0.75 seconds 
2022-05-02 18:22:33.435569: This epoch took 496.104420 s
 
2022-05-02 18:22:33.436046: 
epoch:  51 
2022-05-02 18:30:07.887625: train loss : -0.7332 
2022-05-02 18:30:40.676154: validation loss: -0.7312 
2022-05-02 18:30:40.676759: Average global foreground Dice: [0.7895] 
2022-05-02 18:30:40.676916: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 18:30:41.656824: lr: 0.002945 
2022-05-02 18:30:41.657154: This epoch took 488.221024 s
 
2022-05-02 18:30:41.657246: 
epoch:  52 
2022-05-02 18:38:31.078703: train loss : -0.7372 
2022-05-02 18:39:04.842072: validation loss: -0.7538 
2022-05-02 18:39:04.842711: Average global foreground Dice: [0.8003] 
2022-05-02 18:39:04.842883: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 18:39:05.823232: lr: 0.002798 
2022-05-02 18:39:05.873017: saving checkpoint... 
2022-05-02 18:39:06.427136: done, saving took 0.60 seconds 
2022-05-02 18:39:06.430651: This epoch took 504.773337 s
 
2022-05-02 18:39:06.430879: 
epoch:  53 
2022-05-02 18:46:36.826222: train loss : -0.7481 
2022-05-02 18:47:10.200127: validation loss: -0.7575 
2022-05-02 18:47:10.200733: Average global foreground Dice: [0.8007] 
2022-05-02 18:47:10.200886: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 18:47:11.198261: lr: 0.002649 
2022-05-02 18:47:11.247059: saving checkpoint... 
2022-05-02 18:47:11.812043: done, saving took 0.61 seconds 
2022-05-02 18:47:11.815960: This epoch took 485.384993 s
 
2022-05-02 18:47:11.816148: 
epoch:  54 
2022-05-02 18:54:44.326569: train loss : -0.7092 
2022-05-02 18:55:17.523647: validation loss: -0.7301 
2022-05-02 18:55:17.524243: Average global foreground Dice: [0.7997] 
2022-05-02 18:55:17.524390: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 18:55:18.496853: lr: 0.0025 
2022-05-02 18:55:18.545696: saving checkpoint... 
2022-05-02 18:55:19.087419: done, saving took 0.59 seconds 
2022-05-02 18:55:19.090691: This epoch took 487.274472 s
 
2022-05-02 18:55:19.090865: 
epoch:  55 
2022-05-02 19:03:03.167877: train loss : -0.7180 
2022-05-02 19:03:37.687621: validation loss: -0.7559 
2022-05-02 19:03:37.688327: Average global foreground Dice: [0.8088] 
2022-05-02 19:03:37.688488: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 19:03:38.820587: lr: 0.002349 
2022-05-02 19:03:38.870146: saving checkpoint... 
2022-05-02 19:03:39.324798: done, saving took 0.50 seconds 
2022-05-02 19:03:39.328760: This epoch took 500.237819 s
 
2022-05-02 19:03:39.329031: 
epoch:  56 
2022-05-02 19:11:11.710131: train loss : -0.7234 
2022-05-02 19:11:44.725090: validation loss: -0.7617 
2022-05-02 19:11:44.725709: Average global foreground Dice: [0.8055] 
2022-05-02 19:11:44.725855: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 19:11:45.693364: lr: 0.002198 
2022-05-02 19:11:45.743953: saving checkpoint... 
2022-05-02 19:11:46.193052: done, saving took 0.50 seconds 
2022-05-02 19:11:46.196080: This epoch took 486.866949 s
 
2022-05-02 19:11:46.196256: 
epoch:  57 
2022-05-02 19:18:57.046543: train loss : -0.7243 
2022-05-02 19:19:30.004575: validation loss: -0.7757 
2022-05-02 19:19:30.005165: Average global foreground Dice: [0.8252] 
2022-05-02 19:19:30.005312: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 19:19:30.963594: lr: 0.002045 
2022-05-02 19:19:31.010716: saving checkpoint... 
2022-05-02 19:19:31.520562: done, saving took 0.56 seconds 
2022-05-02 19:19:31.524077: This epoch took 465.327748 s
 
2022-05-02 19:19:31.524259: 
epoch:  58 
2022-05-02 19:26:54.981070: train loss : -0.7237 
2022-05-02 19:27:27.979540: validation loss: -0.7569 
2022-05-02 19:27:27.980170: Average global foreground Dice: [0.8169] 
2022-05-02 19:27:27.980325: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 19:27:28.945260: lr: 0.001891 
2022-05-02 19:27:28.993623: saving checkpoint... 
2022-05-02 19:27:29.487370: done, saving took 0.54 seconds 
2022-05-02 19:27:29.490458: This epoch took 477.966124 s
 
2022-05-02 19:27:29.490633: 
epoch:  59 
2022-05-02 19:34:51.374309: train loss : -0.7289 
2022-05-02 19:35:24.171996: validation loss: -0.7526 
2022-05-02 19:35:24.172632: Average global foreground Dice: [0.7998] 
2022-05-02 19:35:24.172802: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 19:35:25.128884: lr: 0.001735 
2022-05-02 19:35:25.129229: This epoch took 475.638530 s
 
2022-05-02 19:35:25.129319: 
epoch:  60 
2022-05-02 19:43:09.349221: train loss : -0.7350 
2022-05-02 19:43:42.420790: validation loss: -0.7500 
2022-05-02 19:43:42.421442: Average global foreground Dice: [0.8019] 
2022-05-02 19:43:42.421602: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 19:43:43.381223: lr: 0.001578 
2022-05-02 19:43:43.381549: This epoch took 498.252170 s
 
2022-05-02 19:43:43.381626: 
epoch:  61 
2022-05-02 19:51:21.075628: train loss : -0.7222 
2022-05-02 19:51:54.255274: validation loss: -0.7772 
2022-05-02 19:51:54.255863: Average global foreground Dice: [0.8061] 
2022-05-02 19:51:54.256016: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 19:51:55.225165: lr: 0.00142 
2022-05-02 19:51:55.276025: saving checkpoint... 
2022-05-02 19:51:55.997588: done, saving took 0.77 seconds 
2022-05-02 19:51:56.001568: This epoch took 492.619881 s
 
2022-05-02 19:51:56.001723: 
epoch:  62 
2022-05-02 19:59:39.255957: train loss : -0.7171 
2022-05-02 20:00:12.718816: validation loss: -0.7710 
2022-05-02 20:00:12.719455: Average global foreground Dice: [0.8198] 
2022-05-02 20:00:12.719630: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 20:00:13.676987: lr: 0.001259 
2022-05-02 20:00:13.725344: saving checkpoint... 
2022-05-02 20:00:14.454264: done, saving took 0.78 seconds 
2022-05-02 20:00:14.458143: This epoch took 498.456346 s
 
2022-05-02 20:00:14.458333: 
epoch:  63 
2022-05-02 20:08:14.749973: train loss : -0.7238 
2022-05-02 20:08:50.371182: validation loss: -0.7772 
2022-05-02 20:08:50.371823: Average global foreground Dice: [0.8187] 
2022-05-02 20:08:50.371987: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 20:08:51.334170: lr: 0.001096 
2022-05-02 20:08:51.384330: saving checkpoint... 
2022-05-02 20:08:52.149458: done, saving took 0.81 seconds 
2022-05-02 20:08:52.153710: This epoch took 517.695305 s
 
2022-05-02 20:08:52.153920: 
epoch:  64 
2022-05-02 20:16:50.463496: train loss : -0.7344 
2022-05-02 20:17:25.337613: validation loss: -0.7307 
2022-05-02 20:17:25.338303: Average global foreground Dice: [0.8037] 
2022-05-02 20:17:25.338451: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 20:17:26.306458: lr: 0.00093 
2022-05-02 20:17:26.306838: This epoch took 514.152821 s
 
2022-05-02 20:17:26.306935: 
epoch:  65 
2022-05-02 20:25:27.028274: train loss : -0.7397 
2022-05-02 20:26:02.195038: validation loss: -0.7763 
2022-05-02 20:26:02.195760: Average global foreground Dice: [0.8207] 
2022-05-02 20:26:02.195922: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 20:26:03.332730: lr: 0.000761 
2022-05-02 20:26:03.382045: saving checkpoint... 
2022-05-02 20:26:03.787130: done, saving took 0.45 seconds 
2022-05-02 20:26:03.791737: This epoch took 517.484722 s
 
2022-05-02 20:26:03.791966: 
epoch:  66 
2022-05-02 20:34:01.780951: train loss : -0.7200 
2022-05-02 20:34:36.573548: validation loss: -0.7765 
2022-05-02 20:34:36.574236: Average global foreground Dice: [0.8191] 
2022-05-02 20:34:36.574391: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 20:34:37.535371: lr: 0.000587 
2022-05-02 20:34:37.584373: saving checkpoint... 
2022-05-02 20:34:38.004833: done, saving took 0.47 seconds 
2022-05-02 20:34:38.009459: This epoch took 514.217410 s
 
2022-05-02 20:34:38.009670: 
epoch:  67 
2022-05-02 20:42:29.465831: train loss : -0.7364 
2022-05-02 20:43:04.085064: validation loss: -0.7886 
2022-05-02 20:43:04.085691: Average global foreground Dice: [0.838] 
2022-05-02 20:43:04.085863: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 20:43:05.045057: lr: 0.000408 
2022-05-02 20:43:05.093013: saving checkpoint... 
2022-05-02 20:43:05.590121: done, saving took 0.54 seconds 
2022-05-02 20:43:05.593994: This epoch took 507.584230 s
 
2022-05-02 20:43:05.594192: 
epoch:  68 
2022-05-02 20:50:48.711310: train loss : -0.7257 
2022-05-02 20:51:24.138481: validation loss: -0.7733 
2022-05-02 20:51:24.139237: Average global foreground Dice: [0.8306] 
2022-05-02 20:51:24.139464: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 20:51:25.102850: lr: 0.000218 
2022-05-02 20:51:25.152882: saving checkpoint... 
2022-05-02 20:51:25.691155: done, saving took 0.59 seconds 
2022-05-02 20:51:25.695227: This epoch took 500.100960 s
 
2022-05-02 20:51:25.695461: 
epoch:  69 
2022-05-02 20:59:16.320575: train loss : -0.7310 
2022-05-02 20:59:51.293406: validation loss: -0.7702 
2022-05-02 20:59:51.294073: Average global foreground Dice: [0.8114] 
2022-05-02 20:59:51.294220: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-02 20:59:52.254312: lr: 0.0 
2022-05-02 20:59:52.254698: This epoch took 506.559152 s
 
2022-05-02 20:59:52.301793: saving checkpoint... 
2022-05-02 20:59:52.821633: done, saving took 0.57 seconds 
