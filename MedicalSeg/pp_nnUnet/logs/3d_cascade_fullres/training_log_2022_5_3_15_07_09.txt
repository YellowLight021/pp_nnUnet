Starting... 
2022-05-03 15:07:09.793929: TRAINING KEYS:
 odict_keys(['lung_001', 'lung_003', 'lung_004', 'lung_005', 'lung_006', 'lung_009', 'lung_010', 'lung_014', 'lung_015', 'lung_016', 'lung_018', 'lung_020', 'lung_022', 'lung_023', 'lung_025', 'lung_026', 'lung_027', 'lung_028', 'lung_029', 'lung_031', 'lung_033', 'lung_034', 'lung_036', 'lung_037', 'lung_038', 'lung_041', 'lung_042', 'lung_043', 'lung_044', 'lung_045', 'lung_046', 'lung_047', 'lung_048', 'lung_049', 'lung_051', 'lung_053', 'lung_054', 'lung_055', 'lung_057', 'lung_058', 'lung_059', 'lung_061', 'lung_062', 'lung_064', 'lung_065', 'lung_066', 'lung_069', 'lung_070', 'lung_071', 'lung_073', 'lung_074', 'lung_075', 'lung_078', 'lung_079', 'lung_080', 'lung_081', 'lung_083', 'lung_084', 'lung_086', 'lung_092', 'lung_093', 'lung_095', 'lung_096']) 
2022-05-03 15:07:09.794437: VALIDATION KEYS:
 odict_keys(['lung_001', 'lung_003', 'lung_004', 'lung_005', 'lung_006', 'lung_009', 'lung_010', 'lung_014', 'lung_015', 'lung_016', 'lung_018', 'lung_020', 'lung_022', 'lung_023', 'lung_025', 'lung_026', 'lung_027', 'lung_028', 'lung_029', 'lung_031', 'lung_033', 'lung_034', 'lung_036', 'lung_037', 'lung_038', 'lung_041', 'lung_042', 'lung_043', 'lung_044', 'lung_045', 'lung_046', 'lung_047', 'lung_048', 'lung_049', 'lung_051', 'lung_053', 'lung_054', 'lung_055', 'lung_057', 'lung_058', 'lung_059', 'lung_061', 'lung_062', 'lung_064', 'lung_065', 'lung_066', 'lung_069', 'lung_070', 'lung_071', 'lung_073', 'lung_074', 'lung_075', 'lung_078', 'lung_079', 'lung_080', 'lung_081', 'lung_083', 'lung_084', 'lung_086', 'lung_092', 'lung_093', 'lung_095', 'lung_096']) 
2022-05-03 15:07:14.718934: loading checkpoint /home/aistudio/Dataset/nnUnet_trained_models/nnUNet/3d_cascade_fullres/Task006_Lung/nnUNetTrainerV2CascadeFullRes__nnUNetPlansv2.1/all/model_latest.model train= True 
2022-05-03 15:07:15.235357: lr: 0.003238 
2022-05-03 15:07:19.490268: Unable to plot network architecture: 
2022-05-03 15:07:19.490574: No module named 'hiddenlayer' 
2022-05-03 15:07:19.490657: 
printing the network instead:
 
2022-05-03 15:07:19.490705: Generic_UNet(
  (conv_blocks_localization): LayerList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(640, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(512, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(256, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(256, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(128, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(128, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(64, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(64, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(32, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (conv_blocks_context): LayerList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(2, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(32, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(64, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(128, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(128, 256, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(256, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(256, 320, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], stride=[1, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (td): LayerList()
  (tu): LayerList(
    (0): Conv3DTranspose(320, 320, kernel_size=[1, 2, 2], stride=[1, 2, 2], data_format=NCDHW)
    (1): Conv3DTranspose(320, 256, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (2): Conv3DTranspose(256, 128, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (3): Conv3DTranspose(128, 64, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (4): Conv3DTranspose(64, 32, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
  )
  (seg_outputs): LayerList(
    (0): Conv3D(320, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (1): Conv3D(256, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (2): Conv3D(128, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (3): Conv3D(64, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (4): Conv3D(32, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
  )
) 
2022-05-03 15:07:19.493387: 
 
2022-05-03 15:07:19.493573: 
epoch:  50 
2022-05-03 15:40:38.591459: train loss : 0.3275 
2022-05-03 15:42:10.934302: validation loss: 0.3352 
2022-05-03 15:42:10.934927: Average global foreground Dice: [0.6911] 
2022-05-03 15:42:10.935065: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-03 15:42:12.009413: lr: 0.003092 
2022-05-03 15:42:12.146497: saving checkpoint... 
2022-05-03 15:42:12.694199: done, saving took 0.68 seconds 
2022-05-03 15:42:12.698090: This epoch took 2093.204441 s
 
2022-05-03 15:42:12.698267: 
epoch:  51 
2022-05-03 16:17:01.930724: train loss : 0.3089 
2022-05-03 16:18:32.521143: validation loss: 0.3282 
2022-05-03 16:18:32.521772: Average global foreground Dice: [0.7007] 
2022-05-03 16:18:32.521917: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-03 16:18:33.577151: lr: 0.002945 
2022-05-03 16:18:33.615120: saving checkpoint... 
2022-05-03 16:18:34.130791: done, saving took 0.55 seconds 
2022-05-03 16:18:34.135399: This epoch took 2181.437057 s
 
2022-05-03 16:18:34.135604: 
epoch:  52 
2022-05-03 16:53:05.978325: train loss : 0.3347 
2022-05-03 16:54:40.711247: validation loss: 0.2976 
2022-05-03 16:54:40.711967: Average global foreground Dice: [0.7222] 
2022-05-03 16:54:40.712113: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-03 16:54:41.761506: lr: 0.002798 
2022-05-03 16:54:41.799611: saving checkpoint... 
2022-05-03 16:54:42.334954: done, saving took 0.57 seconds 
2022-05-03 16:54:42.338652: This epoch took 2168.202972 s
 
2022-05-03 16:54:42.338831: 
epoch:  53 
2022-05-03 17:29:45.649528: train loss : 0.3255 
2022-05-03 17:31:19.266321: validation loss: 0.3423 
2022-05-03 17:31:19.266979: Average global foreground Dice: [0.68] 
2022-05-03 17:31:19.267145: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-03 17:31:20.325433: lr: 0.002649 
2022-05-03 17:31:20.325803: This epoch took 2197.986901 s
 
2022-05-03 17:31:20.325924: 
epoch:  54 
2022-05-03 18:06:32.554022: train loss : 0.3248 
2022-05-03 18:08:05.106647: validation loss: 0.3221 
2022-05-03 18:08:05.107315: Average global foreground Dice: [0.7165] 
2022-05-03 18:08:05.107458: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-03 18:08:06.162889: lr: 0.0025 
2022-05-03 18:08:06.201636: saving checkpoint... 
2022-05-03 18:08:06.667623: done, saving took 0.50 seconds 
2022-05-03 18:08:06.671756: This epoch took 2206.345727 s
 
2022-05-03 18:08:06.671960: 
epoch:  55 
2022-05-03 18:42:35.627167: train loss : 0.2916 
2022-05-03 18:44:09.011971: validation loss: 0.3097 
2022-05-03 18:44:09.012634: Average global foreground Dice: [0.735] 
2022-05-03 18:44:09.012785: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-03 18:44:10.057072: lr: 0.002349 
2022-05-03 18:44:10.095905: saving checkpoint... 
2022-05-03 18:44:10.613514: done, saving took 0.56 seconds 
2022-05-03 18:44:10.618420: This epoch took 2163.946384 s
 
2022-05-03 18:44:10.618620: 
epoch:  56 
2022-05-03 19:20:37.195157: train loss : 0.3216 
2022-05-03 19:22:02.061153: validation loss: 0.3259 
2022-05-03 19:22:02.061771: Average global foreground Dice: [0.7084] 
2022-05-03 19:22:02.061914: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-03 19:22:03.103109: lr: 0.002198 
2022-05-03 19:22:03.140587: saving checkpoint... 
2022-05-03 19:22:03.454153: done, saving took 0.35 seconds 
2022-05-03 19:22:03.458040: This epoch took 2272.839340 s
 
2022-05-03 19:22:03.458242: 
epoch:  57 
2022-05-03 19:55:05.737392: train loss : 0.3130 
2022-05-03 19:56:27.626690: validation loss: 0.2817 
2022-05-03 19:56:27.627280: Average global foreground Dice: [0.7383] 
2022-05-03 19:56:27.627424: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-03 19:56:28.649580: lr: 0.002045 
2022-05-03 19:56:28.686347: saving checkpoint... 
2022-05-03 19:56:29.001151: done, saving took 0.35 seconds 
2022-05-03 19:56:29.004803: This epoch took 2065.546486 s
 
2022-05-03 19:56:29.004986: 
epoch:  58 
2022-05-03 20:27:36.660549: train loss : 0.2919 
2022-05-03 20:29:02.194139: validation loss: 0.2962 
2022-05-03 20:29:02.194777: Average global foreground Dice: [0.7193] 
2022-05-03 20:29:02.194954: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-03 20:29:03.212492: lr: 0.001891 
2022-05-03 20:29:03.249400: saving checkpoint... 
2022-05-03 20:29:03.612141: done, saving took 0.40 seconds 
2022-05-03 20:29:03.615874: This epoch took 1954.610819 s
 
2022-05-03 20:29:03.616439: 
epoch:  59 
2022-05-03 21:01:13.461262: train loss : 0.3016 
2022-05-03 21:02:40.142763: validation loss: 0.3120 
2022-05-03 21:02:40.143384: Average global foreground Dice: [0.6887] 
2022-05-03 21:02:40.143579: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-03 21:02:41.164721: lr: 0.001735 
2022-05-03 21:02:41.165049: This epoch took 2017.548504 s
 
2022-05-03 21:02:41.165139: 
epoch:  60 
2022-05-03 21:35:08.839326: train loss : 0.3351 
2022-05-03 21:36:38.799905: validation loss: 0.2773 
2022-05-03 21:36:38.800526: Average global foreground Dice: [0.7464] 
2022-05-03 21:36:38.800692: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-03 21:36:39.839692: lr: 0.001578 
2022-05-03 21:36:39.876917: saving checkpoint... 
2022-05-03 21:36:40.319115: done, saving took 0.48 seconds 
2022-05-03 21:36:40.323036: This epoch took 2039.157832 s
 
2022-05-03 21:36:40.323208: 
epoch:  61 
2022-05-03 22:10:13.132141: train loss : 0.3263 
2022-05-03 22:11:42.300196: validation loss: 0.2712 
2022-05-03 22:11:42.300785: Average global foreground Dice: [0.729] 
2022-05-03 22:11:42.300905: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-03 22:11:43.319691: lr: 0.00142 
2022-05-03 22:11:43.358187: saving checkpoint... 
2022-05-03 22:11:43.823563: done, saving took 0.50 seconds 
2022-05-03 22:11:43.827219: This epoch took 2103.503939 s
 
2022-05-03 22:11:43.827757: 
epoch:  62 
2022-05-03 22:45:00.980344: train loss : 0.2956 
2022-05-03 22:46:31.125497: validation loss: 0.2396 
2022-05-03 22:46:31.126082: Average global foreground Dice: [0.7713] 
2022-05-03 22:46:31.126225: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-03 22:46:32.167459: lr: 0.001259 
2022-05-03 22:46:32.204536: saving checkpoint... 
2022-05-03 22:46:32.703844: done, saving took 0.54 seconds 
2022-05-03 22:46:32.707341: This epoch took 2088.879469 s
 
2022-05-03 22:46:32.707489: 
epoch:  63 
2022-05-03 23:18:46.400556: train loss : 0.3221 
2022-05-03 23:20:17.717634: validation loss: 0.2581 
2022-05-03 23:20:17.718225: Average global foreground Dice: [0.7909] 
2022-05-03 23:20:17.718369: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-03 23:20:18.758400: lr: 0.001096 
2022-05-03 23:20:18.795335: saving checkpoint... 
2022-05-03 23:20:19.282448: done, saving took 0.52 seconds 
2022-05-03 23:20:19.287142: This epoch took 2026.579586 s
 
2022-05-03 23:20:19.287459: 
epoch:  64 
2022-05-03 23:53:52.962445: train loss : 0.3004 
2022-05-03 23:55:25.068904: validation loss: 0.3197 
2022-05-03 23:55:25.069497: Average global foreground Dice: [0.7017] 
2022-05-03 23:55:25.069631: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-03 23:55:26.105238: lr: 0.00093 
2022-05-03 23:55:26.105550: This epoch took 2106.818021 s
 
2022-05-03 23:55:26.105628: 
epoch:  65 
2022-05-04 00:27:53.566639: train loss : 0.2848 
2022-05-04 00:29:23.399553: validation loss: 0.2790 
2022-05-04 00:29:23.400136: Average global foreground Dice: [0.7483] 
2022-05-04 00:29:23.400269: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-04 00:29:24.433102: lr: 0.000761 
2022-05-04 00:29:24.469884: saving checkpoint... 
2022-05-04 00:29:24.962245: done, saving took 0.53 seconds 
2022-05-04 00:29:24.966002: This epoch took 2038.860312 s
 
2022-05-04 00:29:24.966166: 
epoch:  66 
2022-05-04 01:02:08.275084: train loss : 0.3059 
2022-05-04 01:03:30.127115: validation loss: 0.2863 
2022-05-04 01:03:30.127712: Average global foreground Dice: [0.7604] 
2022-05-04 01:03:30.127900: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-04 01:03:31.155709: lr: 0.000587 
2022-05-04 01:03:31.192014: saving checkpoint... 
2022-05-04 01:03:31.438953: done, saving took 0.28 seconds 
2022-05-04 01:03:31.442603: This epoch took 2046.476368 s
 
2022-05-04 01:03:31.443342: 
epoch:  67 
2022-05-04 01:35:42.512283: train loss : 0.2752 
2022-05-04 01:37:03.851832: validation loss: 0.3326 
2022-05-04 01:37:03.852424: Average global foreground Dice: [0.7251] 
2022-05-04 01:37:03.852552: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-04 01:37:04.873266: lr: 0.000408 
2022-05-04 01:37:04.873562: This epoch took 2013.430121 s
 
2022-05-04 01:37:04.873637: 
epoch:  68 
2022-05-04 02:10:25.766171: train loss : 0.2955 
2022-05-04 02:11:48.057318: validation loss: 0.3197 
2022-05-04 02:11:48.057890: Average global foreground Dice: [0.7374] 
2022-05-04 02:11:48.058040: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-04 02:11:49.078953: lr: 0.000218 
2022-05-04 02:11:49.115017: saving checkpoint... 
2022-05-04 02:11:49.370479: done, saving took 0.29 seconds 
2022-05-04 02:11:49.374016: This epoch took 2084.500314 s
 
2022-05-04 02:11:49.374204: 
epoch:  69 
2022-05-04 02:43:55.169989: train loss : 0.2721 
2022-05-04 02:45:18.683331: validation loss: 0.2810 
2022-05-04 02:45:18.683969: Average global foreground Dice: [0.6664] 
2022-05-04 02:45:18.684125: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-04 02:45:19.704305: lr: 0.0 
2022-05-04 02:45:19.704613: This epoch took 2010.330341 s
 
2022-05-04 02:45:19.740430: saving checkpoint... 
2022-05-04 02:45:20.022050: done, saving took 0.32 seconds 
2022-05-04 08:30:00.290828: finished prediction 
2022-05-04 08:30:00.291239: evaluation of raw predictions 
2022-05-04 08:31:12.310999: determining postprocessing 
