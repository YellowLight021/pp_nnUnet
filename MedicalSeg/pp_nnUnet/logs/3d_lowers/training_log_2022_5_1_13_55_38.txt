Starting... 
2022-05-01 13:55:38.511517: TRAINING KEYS:
 odict_keys(['lung_001', 'lung_003', 'lung_004', 'lung_005', 'lung_006', 'lung_009', 'lung_010', 'lung_014', 'lung_015', 'lung_016', 'lung_018', 'lung_020', 'lung_022', 'lung_023', 'lung_025', 'lung_026', 'lung_027', 'lung_028', 'lung_029', 'lung_031', 'lung_033', 'lung_034', 'lung_036', 'lung_037', 'lung_038', 'lung_041', 'lung_042', 'lung_043', 'lung_044', 'lung_045', 'lung_046', 'lung_047', 'lung_048', 'lung_049', 'lung_051', 'lung_053', 'lung_054', 'lung_055', 'lung_057', 'lung_058', 'lung_059', 'lung_061', 'lung_062', 'lung_064', 'lung_065', 'lung_066', 'lung_069', 'lung_070', 'lung_071', 'lung_073', 'lung_074', 'lung_075', 'lung_078', 'lung_079', 'lung_080', 'lung_081', 'lung_083', 'lung_084', 'lung_086', 'lung_092', 'lung_093', 'lung_095', 'lung_096']) 
2022-05-01 13:55:38.512111: VALIDATION KEYS:
 odict_keys(['lung_001', 'lung_003', 'lung_004', 'lung_005', 'lung_006', 'lung_009', 'lung_010', 'lung_014', 'lung_015', 'lung_016', 'lung_018', 'lung_020', 'lung_022', 'lung_023', 'lung_025', 'lung_026', 'lung_027', 'lung_028', 'lung_029', 'lung_031', 'lung_033', 'lung_034', 'lung_036', 'lung_037', 'lung_038', 'lung_041', 'lung_042', 'lung_043', 'lung_044', 'lung_045', 'lung_046', 'lung_047', 'lung_048', 'lung_049', 'lung_051', 'lung_053', 'lung_054', 'lung_055', 'lung_057', 'lung_058', 'lung_059', 'lung_061', 'lung_062', 'lung_064', 'lung_065', 'lung_066', 'lung_069', 'lung_070', 'lung_071', 'lung_073', 'lung_074', 'lung_075', 'lung_078', 'lung_079', 'lung_080', 'lung_081', 'lung_083', 'lung_084', 'lung_086', 'lung_092', 'lung_093', 'lung_095', 'lung_096']) 
2022-05-01 13:55:44.772872: loading checkpoint /home/aistudio/Dataset/nnUnet_trained_models/nnUNet/3d_lowres/Task006_Lung/nnUNetTrainerV2__nnUNetPlansv2.1/all/model_final_checkpoint.model train= True 
2022-05-01 13:55:45.223872: lr: 0.003238 
2022-05-01 13:55:48.678780: Unable to plot network architecture: 
2022-05-01 13:55:48.679046: No module named 'hiddenlayer' 
2022-05-01 13:55:48.679122: 
printing the network instead:
 
2022-05-01 13:55:48.679179: Generic_UNet(
  (conv_blocks_localization): LayerList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(640, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(512, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(256, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(256, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(128, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(128, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(64, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(64, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(32, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (conv_blocks_context): LayerList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(1, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(32, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(64, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(128, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(128, 256, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(256, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(256, 320, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], stride=[1, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (td): LayerList()
  (tu): LayerList(
    (0): Conv3DTranspose(320, 320, kernel_size=[1, 2, 2], stride=[1, 2, 2], data_format=NCDHW)
    (1): Conv3DTranspose(320, 256, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (2): Conv3DTranspose(256, 128, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (3): Conv3DTranspose(128, 64, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (4): Conv3DTranspose(64, 32, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
  )
  (seg_outputs): LayerList(
    (0): Conv3D(320, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (1): Conv3D(256, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (2): Conv3D(128, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (3): Conv3D(64, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (4): Conv3D(32, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
  )
) 
2022-05-01 13:55:48.682002: 
 
2022-05-01 13:55:48.682195: 
epoch:  50 
2022-05-01 14:22:06.356941: train loss : 0.7597 
2022-05-01 14:23:00.175164: validation loss: 0.7313 
2022-05-01 14:23:00.175736: Average global foreground Dice: [0.3436] 
2022-05-01 14:23:00.175922: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-01 14:23:01.168141: lr: 0.003092 
2022-05-01 14:23:01.300320: saving checkpoint... 
2022-05-01 14:23:01.713329: done, saving took 0.54 seconds 
2022-05-01 14:23:01.717381: This epoch took 1633.035111 s
 
2022-05-01 14:23:01.717665: 
epoch:  51 
2022-05-01 14:47:49.180173: train loss : 0.7112 
2022-05-01 14:48:42.726209: validation loss: 0.6850 
2022-05-01 14:48:42.726815: Average global foreground Dice: [0.4394] 
2022-05-01 14:48:42.726970: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-01 14:48:43.717249: lr: 0.002945 
2022-05-01 14:48:43.746663: saving checkpoint... 
2022-05-01 14:48:44.168638: done, saving took 0.45 seconds 
2022-05-01 14:48:44.172771: This epoch took 1542.455032 s
 
2022-05-01 14:48:44.172948: 
epoch:  52 
2022-05-01 15:11:46.047588: train loss : 0.6873 
2022-05-01 15:12:43.564585: validation loss: 0.6910 
2022-05-01 15:12:43.565163: Average global foreground Dice: [0.4743] 
2022-05-01 15:12:43.565320: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-01 15:12:44.569577: lr: 0.002798 
2022-05-01 15:12:44.599370: saving checkpoint... 
2022-05-01 15:12:45.052772: done, saving took 0.48 seconds 
2022-05-01 15:12:45.056763: This epoch took 1440.883742 s
 
2022-05-01 15:12:45.056952: 
epoch:  53 
2022-05-01 15:36:28.217243: train loss : 0.6841 
2022-05-01 15:37:36.629977: validation loss: 0.6801 
2022-05-01 15:37:36.630551: Average global foreground Dice: [0.4554] 
2022-05-01 15:37:36.630721: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-01 15:37:37.868890: lr: 0.002649 
2022-05-01 15:37:37.911294: saving checkpoint... 
2022-05-01 15:37:38.468063: done, saving took 0.60 seconds 
2022-05-01 15:37:38.472858: This epoch took 1493.415734 s
 
2022-05-01 15:37:38.473034: 
epoch:  54 
2022-05-01 16:02:26.049679: train loss : 0.6637 
2022-05-01 16:03:23.793591: validation loss: 0.6502 
2022-05-01 16:03:23.794184: Average global foreground Dice: [0.4598] 
2022-05-01 16:03:23.794338: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-01 16:03:24.796611: lr: 0.0025 
2022-05-01 16:03:24.825973: saving checkpoint... 
2022-05-01 16:03:25.272983: done, saving took 0.48 seconds 
2022-05-01 16:03:25.277144: This epoch took 1546.804040 s
 
2022-05-01 16:03:25.277333: 
epoch:  55 
2022-05-01 16:26:57.807995: train loss : 0.6615 
2022-05-01 16:28:02.326581: validation loss: 0.6387 
2022-05-01 16:28:02.327480: Average global foreground Dice: [0.5078] 
2022-05-01 16:28:02.327877: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-01 16:28:03.833957: lr: 0.002349 
2022-05-01 16:28:03.880274: saving checkpoint... 
2022-05-01 16:28:04.469268: done, saving took 0.63 seconds 
2022-05-01 16:28:04.473977: This epoch took 1479.196572 s
 
2022-05-01 16:28:04.474682: 
epoch:  56 
2022-05-01 16:51:01.134735: train loss : 0.6435 
2022-05-01 16:51:58.934389: validation loss: 0.6858 
2022-05-01 16:51:58.934994: Average global foreground Dice: [0.3754] 
2022-05-01 16:51:58.935154: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-01 16:51:59.945301: lr: 0.002198 
2022-05-01 16:51:59.945642: This epoch took 1435.470724 s
 
2022-05-01 16:51:59.945720: 
epoch:  57 
2022-05-01 17:16:53.500555: train loss : 0.6392 
2022-05-01 17:17:50.911847: validation loss: 0.5960 
2022-05-01 17:17:50.912410: Average global foreground Dice: [0.5055] 
2022-05-01 17:17:50.912555: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-01 17:17:51.920425: lr: 0.002045 
2022-05-01 17:17:51.949905: saving checkpoint... 
2022-05-01 17:17:52.392340: done, saving took 0.47 seconds 
2022-05-01 17:17:52.396314: This epoch took 1552.450532 s
 
2022-05-01 17:17:52.396501: 
epoch:  58 
2022-05-01 17:40:25.915837: train loss : 0.5968 
2022-05-01 17:41:23.501376: validation loss: 0.5789 
2022-05-01 17:41:23.501974: Average global foreground Dice: [0.5383] 
2022-05-01 17:41:23.502114: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-01 17:41:24.513729: lr: 0.001891 
2022-05-01 17:41:24.542896: saving checkpoint... 
2022-05-01 17:41:24.988530: done, saving took 0.47 seconds 
2022-05-01 17:41:24.992728: This epoch took 1412.596154 s
 
2022-05-01 17:41:24.992902: 
epoch:  59 
