Starting... 
2022-05-23 10:04:19.216064: Using splits from existing split file: /home/aistudio/nnLung/nnUnet_preprocessed/Task006_Lung/splits_final.pkl 
2022-05-23 10:04:19.216464: The split file contains 5 splits. 
2022-05-23 10:04:19.216549: Desired fold for training: 0 
2022-05-23 10:04:19.216599: This split has 50 training and 13 validation cases. 
2022-05-23 10:04:19.447022: TRAINING KEYS:
 odict_keys(['lung_001', 'lung_003', 'lung_004', 'lung_005', 'lung_009', 'lung_014', 'lung_015', 'lung_016', 'lung_018', 'lung_020', 'lung_022', 'lung_023', 'lung_025', 'lung_026', 'lung_027', 'lung_028', 'lung_029', 'lung_031', 'lung_036', 'lung_037', 'lung_038', 'lung_043', 'lung_044', 'lung_045', 'lung_047', 'lung_049', 'lung_051', 'lung_053', 'lung_054', 'lung_055', 'lung_057', 'lung_058', 'lung_061', 'lung_062', 'lung_064', 'lung_069', 'lung_071', 'lung_073', 'lung_074', 'lung_075', 'lung_078', 'lung_080', 'lung_081', 'lung_083', 'lung_084', 'lung_086', 'lung_092', 'lung_093', 'lung_095', 'lung_096']) 
2022-05-23 10:04:19.447269: VALIDATION KEYS:
 odict_keys(['lung_006', 'lung_010', 'lung_033', 'lung_034', 'lung_041', 'lung_042', 'lung_046', 'lung_048', 'lung_059', 'lung_065', 'lung_066', 'lung_070', 'lung_079']) 
2022-05-23 10:04:23.233327: loading checkpoint /home/aistudio/nnLung/nnUnet_trained_models/nnUNet/3d_cascade_fullres/Task006_Lung/nnUNetTrainerV2CascadeFullRes__nnUNetPlansv2.1/fold_0/model_latest.model train= True 
2022-05-23 10:04:23.537951: lr: 0.009549 
2022-05-23 10:04:27.099077: Unable to plot network architecture: 
2022-05-23 10:04:27.099301: No module named 'hiddenlayer' 
2022-05-23 10:04:27.099371: 
printing the network instead:
 
2022-05-23 10:04:27.099429: Generic_UNet(
  (conv_blocks_localization): LayerList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(640, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(512, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(256, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(256, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(128, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(128, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(64, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(64, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(32, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (conv_blocks_context): LayerList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(2, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(32, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(64, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(128, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(128, 256, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(256, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(256, 320, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], stride=[1, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (td): LayerList()
  (tu): LayerList(
    (0): Conv3DTranspose(320, 320, kernel_size=[1, 2, 2], stride=[1, 2, 2], data_format=NCDHW)
    (1): Conv3DTranspose(320, 256, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (2): Conv3DTranspose(256, 128, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (3): Conv3DTranspose(128, 64, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (4): Conv3DTranspose(64, 32, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
  )
  (seg_outputs): LayerList(
    (0): Conv3D(320, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (1): Conv3D(256, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (2): Conv3D(128, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (3): Conv3D(64, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (4): Conv3D(32, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
  )
) 
2022-05-23 10:04:27.101984: 
 
2022-05-23 10:04:27.102181: 
epoch:  50 
2022-05-23 10:36:21.749314: train loss : 0.3298 
2022-05-23 10:38:00.522983: validation loss: 0.3622 
2022-05-23 10:38:00.523613: Average global foreground Dice: [0.6561] 
2022-05-23 10:38:00.523770: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 10:38:01.546715: lr: 0.00954 
2022-05-23 10:38:01.547043: saving scheduled checkpoint file... 
2022-05-23 10:38:01.665883: saving checkpoint... 
2022-05-23 10:38:03.050681: done, saving took 1.50 seconds 
2022-05-23 10:38:03.098585: done 
2022-05-23 10:38:03.098971: This epoch took 2015.996700 s
 
2022-05-23 10:38:03.099066: 
epoch:  51 
2022-05-23 11:13:14.139767: train loss : 0.3448 
2022-05-23 11:14:50.999685: validation loss: 0.3711 
2022-05-23 11:14:51.000317: Average global foreground Dice: [0.6311] 
2022-05-23 11:14:51.000454: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 11:14:52.010518: lr: 0.009531 
2022-05-23 11:14:52.010805: saving scheduled checkpoint file... 
2022-05-23 11:14:52.037625: saving checkpoint... 
2022-05-23 11:14:53.014563: done, saving took 1.00 seconds 
2022-05-23 11:14:53.089443: done 
2022-05-23 11:14:53.089880: This epoch took 2209.990733 s
 
2022-05-23 11:14:53.089983: 
epoch:  52 
2022-05-23 11:48:04.241122: train loss : 0.2959 
2022-05-23 11:49:40.964735: validation loss: 0.3211 
2022-05-23 11:49:40.965420: Average global foreground Dice: [0.7016] 
2022-05-23 11:49:40.965569: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 11:49:41.985979: lr: 0.009522 
2022-05-23 11:49:41.986294: saving scheduled checkpoint file... 
2022-05-23 11:49:42.012830: saving checkpoint... 
2022-05-23 11:49:43.061854: done, saving took 1.08 seconds 
2022-05-23 11:49:43.065319: done 
2022-05-23 11:49:43.065601: This epoch took 2089.975551 s
 
2022-05-23 11:49:43.065678: 
epoch:  53 
2022-05-23 12:23:31.176882: train loss : 0.3141 
2022-05-23 12:25:06.019494: validation loss: 0.3795 
2022-05-23 12:25:06.020197: Average global foreground Dice: [0.5706] 
2022-05-23 12:25:06.020376: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 12:25:07.023913: lr: 0.009513 
2022-05-23 12:25:07.024361: saving scheduled checkpoint file... 
2022-05-23 12:25:07.053175: saving checkpoint... 
2022-05-23 12:25:08.090018: done, saving took 1.07 seconds 
2022-05-23 12:25:08.093115: done 
2022-05-23 12:25:08.093376: This epoch took 2125.027636 s
 
2022-05-23 12:25:08.093445: 
epoch:  54 
2022-05-23 12:59:08.059831: train loss : 0.3333 
2022-05-23 13:00:41.003717: validation loss: 0.3125 
2022-05-23 13:00:41.004295: Average global foreground Dice: [0.7] 
2022-05-23 13:00:41.004426: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 13:00:42.033237: lr: 0.009504 
2022-05-23 13:00:42.033561: saving scheduled checkpoint file... 
2022-05-23 13:00:42.067540: saving checkpoint... 
2022-05-23 13:00:43.208510: done, saving took 1.17 seconds 
2022-05-23 13:00:43.212528: done 
2022-05-23 13:00:43.212732: This epoch took 2135.119228 s
 
2022-05-23 13:00:43.212792: 
epoch:  55 
2022-05-23 13:34:37.913371: train loss : 0.3012 
2022-05-23 13:36:10.435359: validation loss: 0.3842 
2022-05-23 13:36:10.436063: Average global foreground Dice: [0.6183] 
2022-05-23 13:36:10.436247: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 13:36:11.477052: lr: 0.009495 
2022-05-23 13:36:11.477374: saving scheduled checkpoint file... 
2022-05-23 13:36:11.505493: saving checkpoint... 
2022-05-23 13:36:12.463004: done, saving took 0.99 seconds 
2022-05-23 13:36:12.466197: done 
2022-05-23 13:36:12.466459: This epoch took 2129.253609 s
 
2022-05-23 13:36:12.466530: 
epoch:  56 
2022-05-23 14:09:49.684317: train loss : 0.2980 
2022-05-23 14:11:20.158103: validation loss: 0.3694 
2022-05-23 14:11:20.158819: Average global foreground Dice: [0.6229] 
2022-05-23 14:11:20.159386: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 14:11:21.190961: lr: 0.009486 
2022-05-23 14:11:21.191288: saving scheduled checkpoint file... 
2022-05-23 14:11:21.219148: saving checkpoint... 
2022-05-23 14:11:22.350971: done, saving took 1.16 seconds 
2022-05-23 14:11:22.357401: done 
2022-05-23 14:11:22.357828: This epoch took 2109.891223 s
 
2022-05-23 14:11:22.358009: 
epoch:  57 
2022-05-23 14:45:43.044224: train loss : 0.3071 
2022-05-23 14:47:15.262186: validation loss: 0.3094 
2022-05-23 14:47:15.262859: Average global foreground Dice: [0.6741] 
2022-05-23 14:47:15.262997: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 14:47:16.319268: lr: 0.009476 
2022-05-23 14:47:16.319545: saving scheduled checkpoint file... 
2022-05-23 14:47:16.353833: saving checkpoint... 
2022-05-23 14:47:17.284233: done, saving took 0.96 seconds 
2022-05-23 14:47:17.287337: done 
2022-05-23 14:47:17.287562: This epoch took 2154.929434 s
 
2022-05-23 14:47:17.287626: 
epoch:  58 
2022-05-23 15:21:06.507948: train loss : 0.3022 
2022-05-23 15:22:45.319567: validation loss: 0.3091 
2022-05-23 15:22:45.321007: Average global foreground Dice: [0.597] 
2022-05-23 15:22:45.321188: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 15:22:46.352507: lr: 0.009467 
2022-05-23 15:22:46.352872: saving scheduled checkpoint file... 
2022-05-23 15:22:46.383099: saving checkpoint... 
2022-05-23 15:22:47.289314: done, saving took 0.94 seconds 
2022-05-23 15:22:47.293064: done 
2022-05-23 15:22:47.293270: This epoch took 2130.005585 s
 
2022-05-23 15:22:47.293331: 
epoch:  59 
2022-05-23 15:56:28.051856: train loss : 0.3444 
2022-05-23 15:57:59.762878: validation loss: 0.3331 
2022-05-23 15:57:59.763569: Average global foreground Dice: [0.6512] 
2022-05-23 15:57:59.763726: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 15:58:00.824698: lr: 0.009458 
2022-05-23 15:58:00.825004: saving scheduled checkpoint file... 
2022-05-23 15:58:00.859973: saving checkpoint... 
2022-05-23 15:58:02.203209: done, saving took 1.38 seconds 
2022-05-23 15:58:02.206282: done 
2022-05-23 15:58:02.206500: This epoch took 2114.913113 s
 
2022-05-23 15:58:02.206561: 
epoch:  60 
2022-05-23 16:31:04.788877: train loss : 0.2983 
2022-05-23 16:32:40.135337: validation loss: 0.3000 
2022-05-23 16:32:40.136675: Average global foreground Dice: [0.7297] 
2022-05-23 16:32:40.136924: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 16:32:41.154819: lr: 0.009449 
2022-05-23 16:32:41.155153: saving scheduled checkpoint file... 
2022-05-23 16:32:41.182509: saving checkpoint... 
2022-05-23 16:32:42.226018: done, saving took 1.07 seconds 
2022-05-23 16:32:42.228990: done 
2022-05-23 16:32:42.229189: This epoch took 2080.022569 s
 
2022-05-23 16:32:42.229252: 
epoch:  61 
2022-05-23 17:05:59.931446: train loss : 0.3128 
2022-05-23 17:07:31.575011: validation loss: 0.3139 
2022-05-23 17:07:31.575607: Average global foreground Dice: [0.6701] 
2022-05-23 17:07:31.575729: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 17:07:32.596048: lr: 0.00944 
2022-05-23 17:07:32.596333: saving scheduled checkpoint file... 
2022-05-23 17:07:32.622242: saving checkpoint... 
2022-05-23 17:07:33.557919: done, saving took 0.96 seconds 
2022-05-23 17:07:33.560879: done 
2022-05-23 17:07:33.561110: This epoch took 2091.331800 s
 
2022-05-23 17:07:33.561174: 
epoch:  62 
2022-05-23 17:45:23.017520: train loss : 0.2903 
2022-05-23 17:46:55.325832: validation loss: 0.3064 
2022-05-23 17:46:55.326485: Average global foreground Dice: [0.6786] 
2022-05-23 17:46:55.327290: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 17:46:56.333806: lr: 0.009431 
2022-05-23 17:46:56.334116: saving scheduled checkpoint file... 
2022-05-23 17:46:56.361755: saving checkpoint... 
2022-05-23 17:46:57.189547: done, saving took 0.86 seconds 
2022-05-23 17:46:57.192516: done 
2022-05-23 17:46:57.192707: This epoch took 2363.631476 s
 
2022-05-23 17:46:57.192772: 
epoch:  63 
