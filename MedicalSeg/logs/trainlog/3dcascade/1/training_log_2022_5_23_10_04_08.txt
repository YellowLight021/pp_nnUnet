Starting... 
2022-05-23 10:04:08.965292: Using splits from existing split file: /home/aistudio/nnLung/nnUnet_preprocessed/Task006_Lung/splits_final.pkl 
2022-05-23 10:04:08.965773: The split file contains 5 splits. 
2022-05-23 10:04:08.965852: Desired fold for training: 1 
2022-05-23 10:04:08.965902: This split has 50 training and 13 validation cases. 
2022-05-23 10:04:09.191830: TRAINING KEYS:
 odict_keys(['lung_001', 'lung_003', 'lung_005', 'lung_006', 'lung_009', 'lung_010', 'lung_014', 'lung_016', 'lung_018', 'lung_020', 'lung_023', 'lung_025', 'lung_026', 'lung_027', 'lung_028', 'lung_029', 'lung_033', 'lung_034', 'lung_037', 'lung_041', 'lung_042', 'lung_043', 'lung_044', 'lung_045', 'lung_046', 'lung_047', 'lung_048', 'lung_049', 'lung_051', 'lung_054', 'lung_055', 'lung_057', 'lung_058', 'lung_059', 'lung_061', 'lung_065', 'lung_066', 'lung_070', 'lung_073', 'lung_074', 'lung_078', 'lung_079', 'lung_080', 'lung_083', 'lung_084', 'lung_086', 'lung_092', 'lung_093', 'lung_095', 'lung_096']) 
2022-05-23 10:04:09.192080: VALIDATION KEYS:
 odict_keys(['lung_004', 'lung_015', 'lung_022', 'lung_031', 'lung_036', 'lung_038', 'lung_053', 'lung_062', 'lung_064', 'lung_069', 'lung_071', 'lung_075', 'lung_081']) 
2022-05-23 10:04:12.249467: loading checkpoint /home/aistudio/nnLung/nnUnet_trained_models/nnUNet/3d_cascade_fullres/Task006_Lung/nnUNetTrainerV2CascadeFullRes__nnUNetPlansv2.1/fold_1/model_latest.model train= True 
2022-05-23 10:04:12.559945: lr: 0.009549 
2022-05-23 10:04:19.096349: Unable to plot network architecture: 
2022-05-23 10:04:19.096575: No module named 'hiddenlayer' 
2022-05-23 10:04:19.096649: 
printing the network instead:
 
2022-05-23 10:04:19.096701: Generic_UNet(
  (conv_blocks_localization): LayerList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(640, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(512, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(256, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(256, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(128, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(128, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(64, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(64, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(32, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (conv_blocks_context): LayerList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(2, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(32, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(64, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(128, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(128, 256, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(256, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(256, 320, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], stride=[1, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (td): LayerList()
  (tu): LayerList(
    (0): Conv3DTranspose(320, 320, kernel_size=[1, 2, 2], stride=[1, 2, 2], data_format=NCDHW)
    (1): Conv3DTranspose(320, 256, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (2): Conv3DTranspose(256, 128, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (3): Conv3DTranspose(128, 64, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (4): Conv3DTranspose(64, 32, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
  )
  (seg_outputs): LayerList(
    (0): Conv3D(320, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (1): Conv3D(256, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (2): Conv3D(128, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (3): Conv3D(64, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (4): Conv3D(32, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
  )
) 
2022-05-23 10:04:19.099351: 
 
2022-05-23 10:04:19.099533: 
epoch:  50 
2022-05-23 10:36:22.509072: train loss : 0.2850 
2022-05-23 10:38:00.733646: validation loss: 0.5648 
2022-05-23 10:38:00.734271: Average global foreground Dice: [0.3813] 
2022-05-23 10:38:00.734421: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 10:38:01.775512: lr: 0.00954 
2022-05-23 10:38:01.775825: saving scheduled checkpoint file... 
2022-05-23 10:38:01.891878: saving checkpoint... 
2022-05-23 10:38:03.803719: done, saving took 2.03 seconds 
2022-05-23 10:38:03.806720: done 
2022-05-23 10:38:03.806989: This epoch took 2024.707381 s
 
2022-05-23 10:38:03.807057: 
epoch:  51 
2022-05-23 11:12:03.441602: train loss : 0.3243 
2022-05-23 11:13:38.962640: validation loss: 0.5173 
2022-05-23 11:13:38.963285: Average global foreground Dice: [0.3909] 
2022-05-23 11:13:38.963429: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 11:13:39.987125: lr: 0.009531 
2022-05-23 11:13:39.987521: saving scheduled checkpoint file... 
2022-05-23 11:13:40.014853: saving checkpoint... 
2022-05-23 11:13:41.386749: done, saving took 1.40 seconds 
2022-05-23 11:13:41.390842: done 
2022-05-23 11:13:41.391076: This epoch took 2137.583960 s
 
2022-05-23 11:13:41.391140: 
epoch:  52 
2022-05-23 11:48:07.808074: train loss : 0.3064 
2022-05-23 11:49:42.779562: validation loss: 0.5511 
2022-05-23 11:49:42.780179: Average global foreground Dice: [0.3303] 
2022-05-23 11:49:42.780311: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 11:49:43.855035: lr: 0.009522 
2022-05-23 11:49:43.855325: saving scheduled checkpoint file... 
2022-05-23 11:49:43.882585: saving checkpoint... 
2022-05-23 11:49:45.176203: done, saving took 1.32 seconds 
2022-05-23 11:49:45.179456: done 
2022-05-23 11:49:45.179665: This epoch took 2163.788467 s
 
2022-05-23 11:49:45.179730: 
epoch:  53 
2022-05-23 12:22:08.025111: train loss : 0.3054 
2022-05-23 12:23:44.269339: validation loss: 0.5689 
2022-05-23 12:23:44.270044: Average global foreground Dice: [0.4377] 
2022-05-23 12:23:44.270204: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 12:23:45.323037: lr: 0.009513 
2022-05-23 12:23:45.323370: saving scheduled checkpoint file... 
2022-05-23 12:23:45.358953: saving checkpoint... 
2022-05-23 12:23:46.538634: done, saving took 1.22 seconds 
2022-05-23 12:23:46.545547: done 
2022-05-23 12:23:46.573525: saving checkpoint... 
2022-05-23 12:23:47.834280: done, saving took 1.29 seconds 
2022-05-23 12:23:47.838567: This epoch took 2042.658773 s
 
2022-05-23 12:23:47.838751: 
epoch:  54 
2022-05-23 12:56:28.923724: train loss : 0.3055 
2022-05-23 12:58:01.494306: validation loss: 0.5183 
2022-05-23 12:58:01.494924: Average global foreground Dice: [0.5324] 
2022-05-23 12:58:01.495064: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 12:58:02.511565: lr: 0.009504 
2022-05-23 12:58:02.511867: saving scheduled checkpoint file... 
2022-05-23 12:58:02.545918: saving checkpoint... 
2022-05-23 12:58:03.379217: done, saving took 0.87 seconds 
2022-05-23 12:58:03.382317: done 
2022-05-23 12:58:03.410360: saving checkpoint... 
2022-05-23 12:58:04.413982: done, saving took 1.03 seconds 
2022-05-23 12:58:04.438274: This epoch took 2056.599441 s
 
2022-05-23 12:58:04.438481: 
epoch:  55 
2022-05-23 13:32:24.342376: train loss : 0.3219 
2022-05-23 13:33:56.440518: validation loss: 0.5627 
2022-05-23 13:33:56.441148: Average global foreground Dice: [0.4572] 
2022-05-23 13:33:56.441288: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 13:33:57.515611: lr: 0.009495 
2022-05-23 13:33:57.515897: saving scheduled checkpoint file... 
2022-05-23 13:33:57.550620: saving checkpoint... 
2022-05-23 13:33:58.525123: done, saving took 1.01 seconds 
2022-05-23 13:33:58.528262: done 
2022-05-23 13:33:58.556885: saving checkpoint... 
2022-05-23 13:33:59.636863: done, saving took 1.11 seconds 
2022-05-23 13:33:59.639998: This epoch took 2155.201421 s
 
2022-05-23 13:33:59.640214: 
epoch:  56 
2022-05-23 14:08:37.175402: train loss : 0.2823 
2022-05-23 14:10:09.760406: validation loss: 0.5945 
2022-05-23 14:10:09.761158: Average global foreground Dice: [0.4023] 
2022-05-23 14:10:09.761292: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 14:10:10.802788: lr: 0.009486 
2022-05-23 14:10:10.803111: saving scheduled checkpoint file... 
2022-05-23 14:10:10.839199: saving checkpoint... 
2022-05-23 14:10:11.843200: done, saving took 1.04 seconds 
2022-05-23 14:10:11.846474: done 
2022-05-23 14:10:11.846769: This epoch took 2172.206479 s
 
2022-05-23 14:10:11.846845: 
epoch:  57 
2022-05-23 14:43:45.730706: train loss : 0.3005 
2022-05-23 14:45:18.647728: validation loss: 0.4938 
2022-05-23 14:45:18.648354: Average global foreground Dice: [0.4963] 
2022-05-23 14:45:18.648518: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 14:45:19.855844: lr: 0.009476 
2022-05-23 14:45:19.856249: saving scheduled checkpoint file... 
2022-05-23 14:45:19.883248: saving checkpoint... 
2022-05-23 14:45:21.211284: done, saving took 1.35 seconds 
2022-05-23 14:45:21.214499: done 
2022-05-23 14:45:21.243537: saving checkpoint... 
2022-05-23 14:45:22.248508: done, saving took 1.03 seconds 
2022-05-23 14:45:22.251808: This epoch took 2110.404891 s
 
2022-05-23 14:45:22.252034: 
epoch:  58 
2022-05-23 15:19:48.897869: train loss : 0.3096 
2022-05-23 15:21:22.340760: validation loss: 0.5464 
2022-05-23 15:21:22.341388: Average global foreground Dice: [0.5011] 
2022-05-23 15:21:22.341559: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 15:21:23.394572: lr: 0.009467 
2022-05-23 15:21:23.394913: saving scheduled checkpoint file... 
2022-05-23 15:21:23.423250: saving checkpoint... 
2022-05-23 15:21:24.630007: done, saving took 1.23 seconds 
2022-05-23 15:21:24.635882: done 
2022-05-23 15:21:24.665003: saving checkpoint... 
2022-05-23 15:21:25.468897: done, saving took 0.83 seconds 
2022-05-23 15:21:25.472468: This epoch took 2163.220332 s
 
2022-05-23 15:21:25.472718: 
epoch:  59 
2022-05-23 15:54:47.706163: train loss : 0.2810 
2022-05-23 15:56:20.043985: validation loss: 0.5136 
2022-05-23 15:56:20.044648: Average global foreground Dice: [0.4093] 
2022-05-23 15:56:20.044805: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 15:56:21.086100: lr: 0.009458 
2022-05-23 15:56:21.086471: saving scheduled checkpoint file... 
2022-05-23 15:56:21.122291: saving checkpoint... 
2022-05-23 15:56:21.937636: done, saving took 0.85 seconds 
2022-05-23 15:56:21.940963: done 
2022-05-23 15:56:21.941220: This epoch took 2096.468387 s
 
2022-05-23 15:56:21.941314: 
epoch:  60 
2022-05-23 16:29:40.873634: train loss : 0.2795 
2022-05-23 16:31:17.116758: validation loss: 0.5848 
2022-05-23 16:31:17.117444: Average global foreground Dice: [0.4536] 
2022-05-23 16:31:17.117619: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 16:31:18.148745: lr: 0.009449 
2022-05-23 16:31:18.149073: saving scheduled checkpoint file... 
2022-05-23 16:31:18.177184: saving checkpoint... 
2022-05-23 16:31:19.162556: done, saving took 1.01 seconds 
2022-05-23 16:31:19.165817: done 
2022-05-23 16:31:19.202381: saving checkpoint... 
2022-05-23 16:31:20.341543: done, saving took 1.18 seconds 
2022-05-23 16:31:20.348364: This epoch took 2098.406975 s
 
2022-05-23 16:31:20.348619: 
epoch:  61 
2022-05-23 17:05:11.729024: train loss : 0.2810 
2022-05-23 17:06:43.829057: validation loss: 0.5282 
2022-05-23 17:06:43.829859: Average global foreground Dice: [0.4874] 
2022-05-23 17:06:43.830016: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 17:06:44.856367: lr: 0.00944 
2022-05-23 17:06:44.856665: saving scheduled checkpoint file... 
2022-05-23 17:06:44.890722: saving checkpoint... 
2022-05-23 17:06:45.784916: done, saving took 0.93 seconds 
2022-05-23 17:06:45.788102: done 
2022-05-23 17:06:45.815923: saving checkpoint... 
2022-05-23 17:06:46.944756: done, saving took 1.16 seconds 
