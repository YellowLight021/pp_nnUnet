Starting... 
2022-05-24 07:36:09.457987: Using splits from existing split file: /home/aistudio/nnLung/nnUnet_preprocessed/Task006_Lung/splits_final.pkl 
2022-05-24 07:36:09.458560: The split file contains 5 splits. 
2022-05-24 07:36:09.458686: Desired fold for training: 1 
2022-05-24 07:36:09.458767: This split has 50 training and 13 validation cases. 
2022-05-24 07:36:09.653785: TRAINING KEYS:
 odict_keys(['lung_001', 'lung_003', 'lung_005', 'lung_006', 'lung_009', 'lung_010', 'lung_014', 'lung_016', 'lung_018', 'lung_020', 'lung_023', 'lung_025', 'lung_026', 'lung_027', 'lung_028', 'lung_029', 'lung_033', 'lung_034', 'lung_037', 'lung_041', 'lung_042', 'lung_043', 'lung_044', 'lung_045', 'lung_046', 'lung_047', 'lung_048', 'lung_049', 'lung_051', 'lung_054', 'lung_055', 'lung_057', 'lung_058', 'lung_059', 'lung_061', 'lung_065', 'lung_066', 'lung_070', 'lung_073', 'lung_074', 'lung_078', 'lung_079', 'lung_080', 'lung_083', 'lung_084', 'lung_086', 'lung_092', 'lung_093', 'lung_095', 'lung_096']) 
2022-05-24 07:36:09.654111: VALIDATION KEYS:
 odict_keys(['lung_004', 'lung_015', 'lung_022', 'lung_031', 'lung_036', 'lung_038', 'lung_053', 'lung_062', 'lung_064', 'lung_069', 'lung_071', 'lung_075', 'lung_081']) 
2022-05-24 07:36:16.615510: loading checkpoint /home/aistudio/nnLung/nnUnet_trained_models/nnUNet/3d_cascade_fullres/Task006_Lung/nnUNetTrainerV2CascadeFullRes__nnUNetPlansv2.1/fold_1/model_latest.model train= True 
2022-05-24 07:36:17.164610: lr: 0.009341 
2022-05-24 07:36:25.224365: Unable to plot network architecture: 
2022-05-24 07:36:25.224632: No module named 'hiddenlayer' 
2022-05-24 07:36:25.224699: 
printing the network instead:
 
2022-05-24 07:36:25.224745: Generic_UNet(
  (conv_blocks_localization): LayerList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(640, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(512, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(256, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(256, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(128, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(128, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(64, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(64, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(32, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (conv_blocks_context): LayerList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(2, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(32, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(64, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(128, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(128, 256, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(256, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(256, 320, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], stride=[1, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (td): LayerList()
  (tu): LayerList(
    (0): Conv3DTranspose(320, 320, kernel_size=[1, 2, 2], stride=[1, 2, 2], data_format=NCDHW)
    (1): Conv3DTranspose(320, 256, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (2): Conv3DTranspose(256, 128, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (3): Conv3DTranspose(128, 64, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (4): Conv3DTranspose(64, 32, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
  )
  (seg_outputs): LayerList(
    (0): Conv3D(320, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (1): Conv3D(256, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (2): Conv3D(128, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (3): Conv3D(64, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (4): Conv3D(32, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
  )
) 
2022-05-24 07:36:25.227447: 
 
2022-05-24 07:36:25.227633: 
epoch:  73 
2022-05-24 08:12:50.413798: train loss : 0.2296 
2022-05-24 08:14:35.664195: validation loss: 0.5499 
2022-05-24 08:14:35.664857: Average global foreground Dice: [0.4328] 
2022-05-24 08:14:35.665063: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-24 08:14:37.002800: lr: 0.009331 
2022-05-24 08:14:37.003196: saving scheduled checkpoint file... 
2022-05-24 08:14:37.142807: saving checkpoint... 
2022-05-24 08:14:37.612931: done, saving took 0.61 seconds 
2022-05-24 08:14:37.618284: done 
2022-05-24 08:14:37.618594: This epoch took 2292.390887 s
 
2022-05-24 08:14:37.618683: 
epoch:  74 
2022-05-24 08:52:38.030259: train loss : 0.2605 
2022-05-24 08:54:16.230293: validation loss: 0.5961 
2022-05-24 08:54:16.230891: Average global foreground Dice: [0.3365] 
2022-05-24 08:54:16.231029: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-24 08:54:17.271557: lr: 0.009322 
2022-05-24 08:54:17.271854: saving scheduled checkpoint file... 
2022-05-24 08:54:17.300596: saving checkpoint... 
2022-05-24 08:54:17.709048: done, saving took 0.44 seconds 
2022-05-24 08:54:17.711949: done 
2022-05-24 08:54:17.712156: This epoch took 2380.093392 s
 
2022-05-24 08:54:17.712213: 
epoch:  75 
2022-05-24 09:30:41.870378: train loss : 0.2670 
2022-05-24 09:32:18.796535: validation loss: 0.4870 
2022-05-24 09:32:18.797103: Average global foreground Dice: [0.4935] 
2022-05-24 09:32:18.797225: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-24 09:32:19.856899: lr: 0.009313 
2022-05-24 09:32:19.857190: saving scheduled checkpoint file... 
2022-05-24 09:32:19.884688: saving checkpoint... 
2022-05-24 09:32:20.271313: done, saving took 0.41 seconds 
2022-05-24 09:32:20.274134: done 
2022-05-24 09:32:20.274329: This epoch took 2282.562062 s
 
2022-05-24 09:32:20.274387: 
epoch:  76 
2022-05-24 10:10:10.613080: train loss : 0.2698 
2022-05-24 10:11:57.353419: validation loss: 0.5725 
2022-05-24 10:11:57.353998: Average global foreground Dice: [0.4626] 
2022-05-24 10:11:57.354138: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-24 10:11:58.424739: lr: 0.009304 
2022-05-24 10:11:58.425023: saving scheduled checkpoint file... 
2022-05-24 10:11:58.453789: saving checkpoint... 
2022-05-24 10:11:58.933698: done, saving took 0.51 seconds 
2022-05-24 10:11:58.938504: done 
2022-05-24 10:11:58.938785: This epoch took 2378.664340 s
 
2022-05-24 10:11:58.938876: 
epoch:  77 
2022-05-24 10:48:18.626616: train loss : 0.2607 
2022-05-24 10:49:55.652151: validation loss: 0.5131 
2022-05-24 10:49:55.652743: Average global foreground Dice: [0.5823] 
2022-05-24 10:49:55.652878: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-24 10:49:56.691612: lr: 0.009295 
2022-05-24 10:49:56.691879: saving scheduled checkpoint file... 
2022-05-24 10:49:56.719603: saving checkpoint... 
2022-05-24 10:49:57.047979: done, saving took 0.36 seconds 
2022-05-24 10:49:57.052046: done 
2022-05-24 10:49:57.052266: This epoch took 2278.113292 s
 
2022-05-24 10:49:57.052366: 
epoch:  78 
2022-05-24 11:28:21.558683: train loss : 0.2792 
2022-05-24 11:30:00.683295: validation loss: 0.5016 
2022-05-24 11:30:00.683902: Average global foreground Dice: [0.5605] 
2022-05-24 11:30:00.684077: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-24 11:30:01.729820: lr: 0.009286 
2022-05-24 11:30:01.730089: saving scheduled checkpoint file... 
2022-05-24 11:30:01.758603: saving checkpoint... 
2022-05-24 11:30:02.156017: done, saving took 0.43 seconds 
2022-05-24 11:30:02.159276: done 
2022-05-24 11:30:02.159480: This epoch took 2405.107023 s
 
2022-05-24 11:30:02.159542: 
epoch:  79 
2022-05-24 12:09:23.904253: train loss : 0.2341 
2022-05-24 12:11:02.536319: validation loss: 0.6249 
2022-05-24 12:11:02.537055: Average global foreground Dice: [0.3569] 
2022-05-24 12:11:02.537247: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-24 12:11:03.591284: lr: 0.009277 
2022-05-24 12:11:03.591552: saving scheduled checkpoint file... 
2022-05-24 12:11:03.618974: saving checkpoint... 
2022-05-24 12:11:03.976223: done, saving took 0.38 seconds 
2022-05-24 12:11:03.982608: done 
2022-05-24 12:11:03.982884: This epoch took 2461.823284 s
 
2022-05-24 12:11:03.982949: 
epoch:  80 
2022-05-24 12:47:36.930484: train loss : 0.2436 
2022-05-24 12:49:16.251099: validation loss: 0.5144 
2022-05-24 12:49:16.251896: Average global foreground Dice: [0.5757] 
2022-05-24 12:49:16.252165: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-24 12:49:17.302894: lr: 0.009268 
2022-05-24 12:49:17.303163: saving scheduled checkpoint file... 
2022-05-24 12:49:17.330856: saving checkpoint... 
2022-05-24 12:49:17.758800: done, saving took 0.46 seconds 
2022-05-24 12:49:17.762951: done 
2022-05-24 12:49:17.795935: saving checkpoint... 
2022-05-24 12:49:18.240959: done, saving took 0.48 seconds 
2022-05-24 12:49:18.244895: This epoch took 2294.261880 s
 
2022-05-24 12:49:18.245070: 
epoch:  81 
2022-05-24 13:27:48.499294: train loss : 0.2579 
2022-05-24 13:29:44.828109: validation loss: 0.5682 
2022-05-24 13:29:44.828691: Average global foreground Dice: [0.4768] 
2022-05-24 13:29:44.828827: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-24 13:29:45.894800: lr: 0.009259 
2022-05-24 13:29:45.895100: saving scheduled checkpoint file... 
2022-05-24 13:29:45.931631: saving checkpoint... 
2022-05-24 13:29:46.371778: done, saving took 0.48 seconds 
2022-05-24 13:29:46.374726: done 
2022-05-24 13:29:46.411902: saving checkpoint... 
2022-05-24 13:29:46.814021: done, saving took 0.44 seconds 
2022-05-24 13:29:46.817302: This epoch took 2428.572142 s
 
2022-05-24 13:29:46.817513: 
epoch:  82 
2022-05-24 14:07:40.755596: train loss : 0.2336 
2022-05-24 14:09:24.996378: validation loss: 0.5673 
2022-05-24 14:09:24.996946: Average global foreground Dice: [0.3737] 
2022-05-24 14:09:24.997088: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-24 14:09:26.054747: lr: 0.00925 
2022-05-24 14:09:26.055012: saving scheduled checkpoint file... 
2022-05-24 14:09:26.092554: saving checkpoint... 
2022-05-24 14:09:26.538048: done, saving took 0.48 seconds 
2022-05-24 14:09:26.541076: done 
2022-05-24 14:09:26.541277: This epoch took 2379.723695 s
 
2022-05-24 14:09:26.541333: 
epoch:  83 
2022-05-24 14:48:09.159910: train loss : 0.2364 
2022-05-24 14:49:57.932833: validation loss: 0.6222 
2022-05-24 14:49:57.933696: Average global foreground Dice: [0.4565] 
2022-05-24 14:49:57.933922: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-24 14:49:59.036272: lr: 0.009241 
2022-05-24 14:49:59.036603: saving scheduled checkpoint file... 
2022-05-24 14:49:59.076174: saving checkpoint... 
2022-05-24 14:49:59.495111: done, saving took 0.46 seconds 
2022-05-24 14:49:59.499372: done 
2022-05-24 14:49:59.499614: This epoch took 2432.958207 s
 
2022-05-24 14:49:59.499706: 
epoch:  84 
2022-05-24 15:31:05.049245: train loss : 0.2555 
2022-05-24 15:33:14.324863: validation loss: 0.5339 
2022-05-24 15:33:14.325658: Average global foreground Dice: [0.4714] 
2022-05-24 15:33:14.325881: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-24 15:33:15.661017: lr: 0.009232 
2022-05-24 15:33:15.661442: saving scheduled checkpoint file... 
2022-05-24 15:33:15.699557: saving checkpoint... 
2022-05-24 15:33:16.252568: done, saving took 0.59 seconds 
2022-05-24 15:33:16.261434: done 
