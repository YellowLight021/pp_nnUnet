Starting... 
2022-05-23 10:06:09.435078: Using splits from existing split file: /home/aistudio/Dataset/nnUnet_preprocessed/Task006_Lung/splits_final.pkl 
2022-05-23 10:06:09.436024: The split file contains 5 splits. 
2022-05-23 10:06:09.436204: Desired fold for training: 3 
2022-05-23 10:06:09.436301: This split has 51 training and 12 validation cases. 
2022-05-23 10:06:09.663301: TRAINING KEYS:
 odict_keys(['lung_001', 'lung_003', 'lung_004', 'lung_005', 'lung_006', 'lung_009', 'lung_010', 'lung_015', 'lung_022', 'lung_025', 'lung_026', 'lung_031', 'lung_033', 'lung_034', 'lung_036', 'lung_037', 'lung_038', 'lung_041', 'lung_042', 'lung_044', 'lung_045', 'lung_046', 'lung_047', 'lung_048', 'lung_049', 'lung_051', 'lung_053', 'lung_054', 'lung_055', 'lung_059', 'lung_061', 'lung_062', 'lung_064', 'lung_065', 'lung_066', 'lung_069', 'lung_070', 'lung_071', 'lung_073', 'lung_074', 'lung_075', 'lung_078', 'lung_079', 'lung_080', 'lung_081', 'lung_083', 'lung_086', 'lung_092', 'lung_093', 'lung_095', 'lung_096']) 
2022-05-23 10:06:09.663574: VALIDATION KEYS:
 odict_keys(['lung_014', 'lung_016', 'lung_018', 'lung_020', 'lung_023', 'lung_027', 'lung_028', 'lung_029', 'lung_043', 'lung_057', 'lung_058', 'lung_084']) 
2022-05-23 10:06:14.932534: loading checkpoint /home/aistudio/Dataset/nnUnet_trained_models/nnUNet/3d_cascade_fullres/Task006_Lung/nnUNetTrainerV2CascadeFullRes__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model train= True 
2022-05-23 10:06:15.433484: lr: 0.009322 
2022-05-23 10:06:22.608688: Unable to plot network architecture: 
2022-05-23 10:06:22.608978: No module named 'hiddenlayer' 
2022-05-23 10:06:22.609065: 
printing the network instead:
 
2022-05-23 10:06:22.609119: Generic_UNet(
  (conv_blocks_localization): LayerList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(640, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(512, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(256, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(256, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(128, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(128, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(64, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(64, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(32, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (conv_blocks_context): LayerList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(2, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(32, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(64, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(128, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(128, 256, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(256, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(256, 320, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], stride=[1, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (td): LayerList()
  (tu): LayerList(
    (0): Conv3DTranspose(320, 320, kernel_size=[1, 2, 2], stride=[1, 2, 2], data_format=NCDHW)
    (1): Conv3DTranspose(320, 256, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (2): Conv3DTranspose(256, 128, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (3): Conv3DTranspose(128, 64, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (4): Conv3DTranspose(64, 32, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
  )
  (seg_outputs): LayerList(
    (0): Conv3D(320, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (1): Conv3D(256, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (2): Conv3D(128, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (3): Conv3D(64, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (4): Conv3D(32, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
  )
) 
2022-05-23 10:06:22.611881: 
 
2022-05-23 10:06:22.612079: 
epoch:  75 
2022-05-23 10:43:20.945584: train loss : 0.3496 
2022-05-23 10:44:58.047562: validation loss: 0.2902 
2022-05-23 10:44:58.048148: Average global foreground Dice: [0.7093] 
2022-05-23 10:44:58.048300: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 10:44:59.106092: lr: 0.009313 
2022-05-23 10:44:59.106437: saving scheduled checkpoint file... 
2022-05-23 10:44:59.239737: saving checkpoint... 
2022-05-23 10:44:59.766153: done, saving took 0.66 seconds 
2022-05-23 10:44:59.770534: done 
2022-05-23 10:44:59.812078: saving checkpoint... 
2022-05-23 10:45:00.317521: done, saving took 0.55 seconds 
2022-05-23 10:45:00.322130: This epoch took 2317.709974 s
 
2022-05-23 10:45:00.322348: 
epoch:  76 
2022-05-23 11:24:45.690125: train loss : 0.3633 
2022-05-23 11:26:26.365849: validation loss: 0.3571 
2022-05-23 11:26:26.366492: Average global foreground Dice: [0.6471] 
2022-05-23 11:26:26.366657: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 11:26:27.420478: lr: 0.009304 
2022-05-23 11:26:27.420786: saving scheduled checkpoint file... 
2022-05-23 11:26:27.457399: saving checkpoint... 
2022-05-23 11:26:27.866169: done, saving took 0.45 seconds 
2022-05-23 11:26:27.871186: done 
2022-05-23 11:26:27.871451: This epoch took 2487.549028 s
 
2022-05-23 11:26:27.871972: 
epoch:  77 
2022-05-23 12:03:45.072647: train loss : 0.3524 
2022-05-23 12:05:29.316510: validation loss: 0.3277 
2022-05-23 12:05:29.317183: Average global foreground Dice: [0.658] 
2022-05-23 12:05:29.317343: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 12:05:30.381419: lr: 0.009295 
2022-05-23 12:05:30.381755: saving scheduled checkpoint file... 
2022-05-23 12:05:30.420560: saving checkpoint... 
2022-05-23 12:05:30.934362: done, saving took 0.55 seconds 
2022-05-23 12:05:30.938675: done 
2022-05-23 12:05:30.938945: This epoch took 2343.066865 s
 
2022-05-23 12:05:30.939018: 
epoch:  78 
2022-05-23 12:42:47.076170: train loss : 0.3175 
2022-05-23 12:44:24.960606: validation loss: 0.3136 
2022-05-23 12:44:24.961255: Average global foreground Dice: [0.6901] 
2022-05-23 12:44:24.961398: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 12:44:26.037836: lr: 0.009286 
2022-05-23 12:44:26.038125: saving scheduled checkpoint file... 
2022-05-23 12:44:26.073707: saving checkpoint... 
2022-05-23 12:44:26.475481: done, saving took 0.44 seconds 
2022-05-23 12:44:26.478588: done 
2022-05-23 12:44:26.478863: This epoch took 2335.539782 s
 
2022-05-23 12:44:26.478983: 
epoch:  79 
2022-05-23 13:20:57.888580: train loss : 0.3401 
2022-05-23 13:22:35.621174: validation loss: 0.3913 
2022-05-23 13:22:35.621855: Average global foreground Dice: [0.459] 
2022-05-23 13:22:35.622010: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 13:22:36.698988: lr: 0.009277 
2022-05-23 13:22:36.699302: saving scheduled checkpoint file... 
2022-05-23 13:22:36.730026: saving checkpoint... 
2022-05-23 13:22:37.140768: done, saving took 0.44 seconds 
2022-05-23 13:22:37.144071: done 
2022-05-23 13:22:37.144292: This epoch took 2290.665226 s
 
2022-05-23 13:22:37.144354: 
epoch:  80 
2022-05-23 14:00:51.299630: train loss : 0.3370 
2022-05-23 14:02:30.168190: validation loss: 0.4008 
2022-05-23 14:02:30.168832: Average global foreground Dice: [0.585] 
2022-05-23 14:02:30.168997: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 14:02:31.245163: lr: 0.009268 
2022-05-23 14:02:31.245457: saving scheduled checkpoint file... 
2022-05-23 14:02:31.275483: saving checkpoint... 
2022-05-23 14:02:31.760550: done, saving took 0.51 seconds 
2022-05-23 14:02:31.763868: done 
2022-05-23 14:02:31.764058: This epoch took 2394.619642 s
 
2022-05-23 14:02:31.764135: 
epoch:  81 
2022-05-23 14:38:56.967201: train loss : 0.3527 
2022-05-23 14:40:33.748240: validation loss: 0.3851 
2022-05-23 14:40:33.748890: Average global foreground Dice: [0.5268] 
2022-05-23 14:40:33.749046: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 14:40:34.794442: lr: 0.009259 
2022-05-23 14:40:34.794747: saving scheduled checkpoint file... 
2022-05-23 14:40:34.832560: saving checkpoint... 
2022-05-23 14:40:35.149260: done, saving took 0.35 seconds 
2022-05-23 14:40:35.152458: done 
2022-05-23 14:40:35.152659: This epoch took 2283.388462 s
 
2022-05-23 14:40:35.152725: 
epoch:  82 
2022-05-23 15:17:38.478592: train loss : 0.3289 
2022-05-23 15:19:20.495165: validation loss: 0.3747 
2022-05-23 15:19:20.495800: Average global foreground Dice: [0.5345] 
2022-05-23 15:19:20.495948: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 15:19:21.733594: lr: 0.00925 
2022-05-23 15:19:21.733898: saving scheduled checkpoint file... 
2022-05-23 15:19:21.764416: saving checkpoint... 
2022-05-23 15:19:22.080359: done, saving took 0.35 seconds 
2022-05-23 15:19:22.083670: done 
2022-05-23 15:19:22.083911: This epoch took 2326.931127 s
 
2022-05-23 15:19:22.083996: 
epoch:  83 
2022-05-23 15:56:16.482250: train loss : 0.3694 
2022-05-23 15:57:49.929059: validation loss: 0.4780 
2022-05-23 15:57:49.930038: Average global foreground Dice: [0.3532] 
2022-05-23 15:57:49.930323: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 15:57:51.195286: lr: 0.009241 
2022-05-23 15:57:51.195606: saving scheduled checkpoint file... 
2022-05-23 15:57:51.231569: saving checkpoint... 
2022-05-23 15:57:51.618752: done, saving took 0.42 seconds 
2022-05-23 15:57:51.623957: done 
2022-05-23 15:57:51.624284: This epoch took 2309.540196 s
 
2022-05-23 15:57:51.624496: 
epoch:  84 
2022-05-23 16:34:31.196683: train loss : 0.3189 
2022-05-23 16:36:10.329175: validation loss: 0.3192 
2022-05-23 16:36:10.330110: Average global foreground Dice: [0.6332] 
2022-05-23 16:36:10.330376: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 16:36:11.744089: lr: 0.009232 
2022-05-23 16:36:11.744492: saving scheduled checkpoint file... 
2022-05-23 16:36:11.781371: saving checkpoint... 
2022-05-23 16:36:12.320615: done, saving took 0.58 seconds 
2022-05-23 16:36:12.325385: done 
2022-05-23 16:36:12.325653: This epoch took 2300.701008 s
 
2022-05-23 16:36:12.325798: 
epoch:  85 
2022-05-23 17:13:46.455457: train loss : 0.3037 
2022-05-23 17:15:21.805914: validation loss: 0.3251 
2022-05-23 17:15:21.806558: Average global foreground Dice: [0.6697] 
2022-05-23 17:15:21.806718: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-23 17:15:22.852084: lr: 0.009223 
2022-05-23 17:15:22.852396: saving scheduled checkpoint file... 
2022-05-23 17:15:22.888752: saving checkpoint... 
2022-05-23 17:15:23.272032: done, saving took 0.42 seconds 
2022-05-23 17:15:23.277868: done 
