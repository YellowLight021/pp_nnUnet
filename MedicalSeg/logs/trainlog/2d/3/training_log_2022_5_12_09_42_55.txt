Starting... 
2022-05-12 09:42:55.399969: Using splits from existing split file: /home/aistudio/Dataset/nnUnet_preprocessed/Task006_Lung/splits_final.pkl 
2022-05-12 09:42:55.400609: The split file contains 5 splits. 
2022-05-12 09:42:55.400738: Desired fold for training: 3 
2022-05-12 09:42:55.400797: This split has 51 training and 12 validation cases. 
2022-05-12 09:42:55.582005: TRAINING KEYS:
 odict_keys(['lung_001', 'lung_003', 'lung_004', 'lung_005', 'lung_006', 'lung_009', 'lung_010', 'lung_015', 'lung_022', 'lung_025', 'lung_026', 'lung_031', 'lung_033', 'lung_034', 'lung_036', 'lung_037', 'lung_038', 'lung_041', 'lung_042', 'lung_044', 'lung_045', 'lung_046', 'lung_047', 'lung_048', 'lung_049', 'lung_051', 'lung_053', 'lung_054', 'lung_055', 'lung_059', 'lung_061', 'lung_062', 'lung_064', 'lung_065', 'lung_066', 'lung_069', 'lung_070', 'lung_071', 'lung_073', 'lung_074', 'lung_075', 'lung_078', 'lung_079', 'lung_080', 'lung_081', 'lung_083', 'lung_086', 'lung_092', 'lung_093', 'lung_095', 'lung_096']) 
2022-05-12 09:42:55.582221: VALIDATION KEYS:
 odict_keys(['lung_014', 'lung_016', 'lung_018', 'lung_020', 'lung_023', 'lung_027', 'lung_028', 'lung_029', 'lung_043', 'lung_057', 'lung_058', 'lung_084']) 
2022-05-12 09:43:00.117071: loading checkpoint /home/aistudio/Dataset/nnUnet_trained_models/nnUNet/2d/Task006_Lung/nnUNetTrainerV2__nnUNetPlansv2.1/fold_3/model_latest.model train= True 
2022-05-12 09:43:00.721732: lr: 0.003238 
2022-05-12 09:43:02.323825: Unable to plot network architecture: 
2022-05-12 09:43:02.324046: No module named 'hiddenlayer' 
2022-05-12 09:43:02.324136: 
printing the network instead:
 
2022-05-12 09:43:02.324209: Generic_UNet(
  (conv_blocks_localization): LayerList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(960, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(480, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(960, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(480, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(960, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(480, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(512, 256, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(256, 256, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(256, 128, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(128, 128, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(128, 64, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(64, 64, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (6): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(64, 32, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(32, 32, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (conv_blocks_context): LayerList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2D(1, 32, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2D(32, 32, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2D(32, 64, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2D(64, 64, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2D(64, 128, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2D(128, 128, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2D(128, 256, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2D(256, 256, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2D(256, 480, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2D(480, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (5): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2D(480, 480, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2D(480, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (6): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2D(480, 480, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2D(480, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
          (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (7): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(480, 480, kernel_size=[3, 3], stride=[2, 2], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2D(480, 480, kernel_size=[3, 3], padding=[1, 1], data_format=NCHW)
            (instnorm): InstanceNorm2D(num_features=480, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (td): LayerList()
  (tu): LayerList(
    (0): Conv2DTranspose(480, 480, kernel_size=[2, 2], stride=[2, 2], data_format=NCHW)
    (1): Conv2DTranspose(480, 480, kernel_size=[2, 2], stride=[2, 2], data_format=NCHW)
    (2): Conv2DTranspose(480, 480, kernel_size=[2, 2], stride=[2, 2], data_format=NCHW)
    (3): Conv2DTranspose(480, 256, kernel_size=[2, 2], stride=[2, 2], data_format=NCHW)
    (4): Conv2DTranspose(256, 128, kernel_size=[2, 2], stride=[2, 2], data_format=NCHW)
    (5): Conv2DTranspose(128, 64, kernel_size=[2, 2], stride=[2, 2], data_format=NCHW)
    (6): Conv2DTranspose(64, 32, kernel_size=[2, 2], stride=[2, 2], data_format=NCHW)
  )
  (seg_outputs): LayerList(
    (0): Conv2D(480, 2, kernel_size=[1, 1], data_format=NCHW)
    (1): Conv2D(480, 2, kernel_size=[1, 1], data_format=NCHW)
    (2): Conv2D(480, 2, kernel_size=[1, 1], data_format=NCHW)
    (3): Conv2D(256, 2, kernel_size=[1, 1], data_format=NCHW)
    (4): Conv2D(128, 2, kernel_size=[1, 1], data_format=NCHW)
    (5): Conv2D(64, 2, kernel_size=[1, 1], data_format=NCHW)
    (6): Conv2D(32, 2, kernel_size=[1, 1], data_format=NCHW)
  )
) 
2022-05-12 09:43:02.327525: 
 
2022-05-12 09:43:02.327682: 
epoch:  50 
2022-05-12 09:50:23.731349: train loss : -0.4271 
2022-05-12 09:50:57.298746: validation loss: -0.4149 
2022-05-12 09:50:57.299299: Average global foreground Dice: [0.4807] 
2022-05-12 09:50:57.299424: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 09:50:58.274785: lr: 0.003092 
2022-05-12 09:50:58.437763: saving checkpoint... 
2022-05-12 09:50:58.929413: done, saving took 0.65 seconds 
2022-05-12 09:50:58.933295: This epoch took 476.605546 s
 
2022-05-12 09:50:58.933477: 
epoch:  51 
2022-05-12 09:58:28.433748: train loss : -0.4378 
2022-05-12 09:59:01.634662: validation loss: -0.4812 
2022-05-12 09:59:01.635230: Average global foreground Dice: [0.5514] 
2022-05-12 09:59:01.635378: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 09:59:02.602339: lr: 0.002945 
2022-05-12 09:59:02.641073: saving checkpoint... 
2022-05-12 09:59:03.165980: done, saving took 0.56 seconds 
2022-05-12 09:59:03.169507: This epoch took 484.235933 s
 
2022-05-12 09:59:03.169673: 
epoch:  52 
2022-05-12 10:06:20.911784: train loss : -0.4653 
2022-05-12 10:06:53.639820: validation loss: -0.5191 
2022-05-12 10:06:53.640453: Average global foreground Dice: [0.5937] 
2022-05-12 10:06:53.640655: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 10:06:54.617110: lr: 0.002798 
2022-05-12 10:06:54.655436: saving checkpoint... 
2022-05-12 10:06:55.153961: done, saving took 0.54 seconds 
2022-05-12 10:06:55.157861: This epoch took 471.988097 s
 
2022-05-12 10:06:55.158025: 
epoch:  53 
2022-05-12 10:14:22.315697: train loss : -0.4657 
2022-05-12 10:14:55.211770: validation loss: -0.5090 
2022-05-12 10:14:55.212336: Average global foreground Dice: [0.5909] 
2022-05-12 10:14:55.212485: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 10:14:56.183778: lr: 0.002649 
2022-05-12 10:14:56.222352: saving checkpoint... 
2022-05-12 10:14:56.864868: done, saving took 0.68 seconds 
2022-05-12 10:14:56.869304: This epoch took 481.711168 s
 
2022-05-12 10:14:56.869480: 
epoch:  54 
2022-05-12 10:22:31.363191: train loss : -0.4819 
2022-05-12 10:23:04.160018: validation loss: -0.5200 
2022-05-12 10:23:04.160589: Average global foreground Dice: [0.5716] 
2022-05-12 10:23:04.160747: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 10:23:05.133754: lr: 0.0025 
2022-05-12 10:23:05.172555: saving checkpoint... 
2022-05-12 10:23:05.817822: done, saving took 0.68 seconds 
2022-05-12 10:23:05.822291: This epoch took 488.952716 s
 
2022-05-12 10:23:05.822458: 
epoch:  55 
2022-05-12 10:30:27.970983: train loss : -0.5251 
2022-05-12 10:31:00.689599: validation loss: -0.5208 
2022-05-12 10:31:00.690171: Average global foreground Dice: [0.5773] 
2022-05-12 10:31:00.690321: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 10:31:01.790961: lr: 0.002349 
2022-05-12 10:31:01.829248: saving checkpoint... 
2022-05-12 10:31:02.318596: done, saving took 0.53 seconds 
2022-05-12 10:31:02.322975: This epoch took 476.500426 s
 
2022-05-12 10:31:02.323161: 
epoch:  56 
2022-05-12 10:38:17.298043: train loss : -0.5365 
2022-05-12 10:38:50.107885: validation loss: -0.5576 
2022-05-12 10:38:50.108479: Average global foreground Dice: [0.6121] 
2022-05-12 10:38:50.108652: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 10:38:51.072380: lr: 0.002198 
2022-05-12 10:38:51.110951: saving checkpoint... 
2022-05-12 10:38:51.622169: done, saving took 0.55 seconds 
2022-05-12 10:38:51.626479: This epoch took 469.303221 s
 
2022-05-12 10:38:51.626650: 
epoch:  57 
2022-05-12 10:46:14.276971: train loss : -0.5536 
2022-05-12 10:46:47.250586: validation loss: -0.5703 
2022-05-12 10:46:47.251169: Average global foreground Dice: [0.6229] 
2022-05-12 10:46:47.251324: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 10:46:48.259250: lr: 0.002045 
2022-05-12 10:46:48.297721: saving checkpoint... 
2022-05-12 10:46:48.839731: done, saving took 0.58 seconds 
2022-05-12 10:46:48.844412: This epoch took 477.217668 s
 
2022-05-12 10:46:48.844615: 
epoch:  58 
2022-05-12 10:53:54.996726: train loss : -0.5661 
2022-05-12 10:54:27.754894: validation loss: -0.5843 
2022-05-12 10:54:27.755484: Average global foreground Dice: [0.6416] 
2022-05-12 10:54:27.755637: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 10:54:28.724219: lr: 0.001891 
2022-05-12 10:54:28.764273: saving checkpoint... 
2022-05-12 10:54:29.337827: done, saving took 0.61 seconds 
2022-05-12 10:54:29.342398: This epoch took 460.497679 s
 
2022-05-12 10:54:29.342565: 
epoch:  59 
2022-05-12 11:01:46.091665: train loss : -0.5857 
2022-05-12 11:02:18.788246: validation loss: -0.5418 
2022-05-12 11:02:18.788783: Average global foreground Dice: [0.6007] 
2022-05-12 11:02:18.788933: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 11:02:19.748477: lr: 0.001735 
2022-05-12 11:02:19.786875: saving checkpoint... 
2022-05-12 11:02:20.389138: done, saving took 0.64 seconds 
2022-05-12 11:02:20.393615: This epoch took 471.050948 s
 
2022-05-12 11:02:20.393800: 
epoch:  60 
2022-05-12 11:09:31.829498: train loss : -0.5941 
2022-05-12 11:10:04.789421: validation loss: -0.5626 
2022-05-12 11:10:04.790006: Average global foreground Dice: [0.6437] 
2022-05-12 11:10:04.790160: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 11:10:05.760631: lr: 0.001578 
2022-05-12 11:10:05.798874: saving checkpoint... 
2022-05-12 11:10:06.404908: done, saving took 0.64 seconds 
2022-05-12 11:10:06.408839: This epoch took 466.014579 s
 
2022-05-12 11:10:06.409332: 
epoch:  61 
2022-05-12 11:17:36.952042: train loss : -0.5779 
2022-05-12 11:18:09.812058: validation loss: -0.4963 
2022-05-12 11:18:09.812615: Average global foreground Dice: [0.557] 
2022-05-12 11:18:09.812775: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 11:18:10.779523: lr: 0.00142 
2022-05-12 11:18:10.779832: This epoch took 484.370406 s
 
2022-05-12 11:18:10.779927: 
epoch:  62 
2022-05-12 11:25:42.073142: train loss : -0.6017 
2022-05-12 11:26:14.899503: validation loss: -0.5441 
2022-05-12 11:26:14.900094: Average global foreground Dice: [0.6023] 
2022-05-12 11:26:14.900253: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 11:26:15.879959: lr: 0.001259 
2022-05-12 11:26:15.918385: saving checkpoint... 
2022-05-12 11:26:16.552772: done, saving took 0.67 seconds 
2022-05-12 11:26:16.557137: This epoch took 485.777136 s
 
2022-05-12 11:26:16.557297: 
epoch:  63 
2022-05-12 11:33:38.707113: train loss : -0.6133 
2022-05-12 11:34:11.361322: validation loss: -0.4870 
2022-05-12 11:34:11.361904: Average global foreground Dice: [0.5277] 
2022-05-12 11:34:11.362044: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 11:34:12.368144: lr: 0.001096 
2022-05-12 11:34:12.368464: This epoch took 475.811082 s
 
2022-05-12 11:34:12.368582: 
epoch:  64 
2022-05-12 11:41:50.283904: train loss : -0.6230 
2022-05-12 11:42:22.924126: validation loss: -0.5544 
2022-05-12 11:42:22.924701: Average global foreground Dice: [0.617] 
2022-05-12 11:42:22.924860: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 11:42:23.909268: lr: 0.00093 
2022-05-12 11:42:23.948335: saving checkpoint... 
2022-05-12 11:42:24.623667: done, saving took 0.71 seconds 
2022-05-12 11:42:24.628531: This epoch took 492.259865 s
 
2022-05-12 11:42:24.628720: 
epoch:  65 
2022-05-12 11:49:53.224648: train loss : -0.6279 
2022-05-12 11:50:26.333285: validation loss: -0.5984 
2022-05-12 11:50:26.333899: Average global foreground Dice: [0.6774] 
2022-05-12 11:50:26.334063: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 11:50:27.474352: lr: 0.000761 
2022-05-12 11:50:27.512380: saving checkpoint... 
2022-05-12 11:50:27.847453: done, saving took 0.37 seconds 
2022-05-12 11:50:27.851492: This epoch took 483.222665 s
 
2022-05-12 11:50:27.851645: 
epoch:  66 
2022-05-12 11:57:35.484288: train loss : -0.6138 
2022-05-12 11:58:08.625990: validation loss: -0.4484 
2022-05-12 11:58:08.626507: Average global foreground Dice: [0.5128] 
2022-05-12 11:58:08.626667: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 11:58:09.601485: lr: 0.000587 
2022-05-12 11:58:09.601792: This epoch took 461.750053 s
 
2022-05-12 11:58:09.601897: 
epoch:  67 
2022-05-12 12:05:33.408768: train loss : -0.5987 
2022-05-12 12:06:06.728373: validation loss: -0.5651 
2022-05-12 12:06:06.728967: Average global foreground Dice: [0.6249] 
2022-05-12 12:06:06.729153: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 12:06:07.714805: lr: 0.000408 
2022-05-12 12:06:07.715137: This epoch took 478.113160 s
 
2022-05-12 12:06:07.715230: 
epoch:  68 
2022-05-12 12:13:40.349236: train loss : -0.6307 
2022-05-12 12:14:13.546841: validation loss: -0.5443 
2022-05-12 12:14:13.547515: Average global foreground Dice: [0.5905] 
2022-05-12 12:14:13.547698: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 12:14:14.532930: lr: 0.000218 
2022-05-12 12:14:14.533292: This epoch took 486.817996 s
 
2022-05-12 12:14:14.533406: 
epoch:  69 
2022-05-12 12:21:42.994404: train loss : -0.6173 
2022-05-12 12:22:16.709978: validation loss: -0.6177 
2022-05-12 12:22:16.710547: Average global foreground Dice: [0.6651] 
2022-05-12 12:22:16.710705: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 12:22:17.690229: lr: 0.0 
2022-05-12 12:22:17.728859: saving checkpoint... 
2022-05-12 12:22:18.266619: done, saving took 0.58 seconds 
2022-05-12 12:22:18.270039: This epoch took 483.736547 s
 
2022-05-12 12:22:18.312598: saving checkpoint... 
2022-05-12 12:22:18.750609: done, saving took 0.48 seconds 
2022-05-12 12:36:42.183922: finished prediction 
2022-05-12 12:36:42.184397: evaluation of raw predictions 
2022-05-12 12:36:57.231280: determining postprocessing 
