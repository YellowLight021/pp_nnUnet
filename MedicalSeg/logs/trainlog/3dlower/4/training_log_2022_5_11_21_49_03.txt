Starting... 
2022-05-11 21:49:03.283576: Using splits from existing split file: /home/aistudio/Dataset/nnUnet_preprocessed/Task006_Lung/splits_final.pkl 
2022-05-11 21:49:03.284279: The split file contains 5 splits. 
2022-05-11 21:49:03.284384: Desired fold for training: 4 
2022-05-11 21:49:03.284454: This split has 51 training and 12 validation cases. 
2022-05-11 21:49:03.481944: TRAINING KEYS:
 odict_keys(['lung_001', 'lung_004', 'lung_005', 'lung_006', 'lung_009', 'lung_010', 'lung_014', 'lung_015', 'lung_016', 'lung_018', 'lung_020', 'lung_022', 'lung_023', 'lung_026', 'lung_027', 'lung_028', 'lung_029', 'lung_031', 'lung_033', 'lung_034', 'lung_036', 'lung_037', 'lung_038', 'lung_041', 'lung_042', 'lung_043', 'lung_044', 'lung_046', 'lung_047', 'lung_048', 'lung_049', 'lung_053', 'lung_057', 'lung_058', 'lung_059', 'lung_062', 'lung_064', 'lung_065', 'lung_066', 'lung_069', 'lung_070', 'lung_071', 'lung_074', 'lung_075', 'lung_078', 'lung_079', 'lung_080', 'lung_081', 'lung_083', 'lung_084', 'lung_086']) 
2022-05-11 21:49:03.482239: VALIDATION KEYS:
 odict_keys(['lung_003', 'lung_025', 'lung_045', 'lung_051', 'lung_054', 'lung_055', 'lung_061', 'lung_073', 'lung_092', 'lung_093', 'lung_095', 'lung_096']) 
2022-05-11 21:49:08.752445: lr: 0.01 
2022-05-11 21:49:11.607170: Unable to plot network architecture: 
2022-05-11 21:49:11.607382: No module named 'hiddenlayer' 
2022-05-11 21:49:11.607475: 
printing the network instead:
 
2022-05-11 21:49:11.607540: Generic_UNet(
  (conv_blocks_localization): LayerList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(640, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(512, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(256, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(256, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(128, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(128, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(64, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(64, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(32, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (conv_blocks_context): LayerList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(1, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(32, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(64, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(128, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(128, 256, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(256, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(256, 320, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], stride=[1, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (td): LayerList()
  (tu): LayerList(
    (0): Conv3DTranspose(320, 320, kernel_size=[1, 2, 2], stride=[1, 2, 2], data_format=NCDHW)
    (1): Conv3DTranspose(320, 256, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (2): Conv3DTranspose(256, 128, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (3): Conv3DTranspose(128, 64, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (4): Conv3DTranspose(64, 32, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
  )
  (seg_outputs): LayerList(
    (0): Conv3D(320, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (1): Conv3D(256, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (2): Conv3D(128, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (3): Conv3D(64, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (4): Conv3D(32, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
  )
) 
2022-05-11 21:49:11.610445: 
 
2022-05-11 21:49:11.610616: 
epoch:  0 
2022-05-11 22:12:22.363304: train loss : 1.2650 
2022-05-11 22:13:21.172480: validation loss: 1.0840 
2022-05-11 22:13:21.173172: Average global foreground Dice: [0.0028] 
2022-05-11 22:13:21.173329: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-11 22:13:22.176752: lr: 0.009836 
2022-05-11 22:13:22.177161: This epoch took 1450.566466 s
 
2022-05-11 22:13:22.177272: 
epoch:  1 
2022-05-11 22:36:40.094855: train loss : 1.0579 
2022-05-11 22:37:38.722211: validation loss: 1.0400 
2022-05-11 22:37:38.722839: Average global foreground Dice: [0.0002] 
2022-05-11 22:37:38.722988: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-11 22:37:39.676749: lr: 0.009672 
2022-05-11 22:37:39.677103: This epoch took 1457.499732 s
 
2022-05-11 22:37:39.677209: 
epoch:  2 
2022-05-11 23:01:14.115088: train loss : 1.0341 
2022-05-11 23:02:11.977726: validation loss: 1.0262 
2022-05-11 23:02:11.978329: Average global foreground Dice: [0.0] 
2022-05-11 23:02:11.978491: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-11 23:02:12.903608: lr: 0.009508 
2022-05-11 23:02:12.903944: This epoch took 1473.226647 s
 
2022-05-11 23:02:12.904026: 
epoch:  3 
2022-05-11 23:25:26.798913: train loss : 1.0248 
2022-05-11 23:26:25.333628: validation loss: 1.0195 
2022-05-11 23:26:25.334304: Average global foreground Dice: [0.0] 
2022-05-11 23:26:25.334473: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-11 23:26:26.235793: lr: 0.009343 
2022-05-11 23:26:26.236200: This epoch took 1453.332083 s
 
2022-05-11 23:26:26.236299: 
epoch:  4 
2022-05-11 23:49:41.451862: train loss : 1.0202 
2022-05-11 23:50:39.977261: validation loss: 1.0158 
2022-05-11 23:50:39.977960: Average global foreground Dice: [0.0] 
2022-05-11 23:50:39.978130: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-11 23:50:40.918099: lr: 0.009178 
2022-05-11 23:50:40.918472: This epoch took 1454.682101 s
 
2022-05-11 23:50:40.918565: 
epoch:  5 
2022-05-12 00:14:07.146566: train loss : 1.0180 
2022-05-12 00:15:05.139449: validation loss: 1.0131 
2022-05-12 00:15:05.140132: Average global foreground Dice: [0.0] 
2022-05-12 00:15:05.140310: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 00:15:06.032369: lr: 0.009013 
2022-05-12 00:15:06.032723: This epoch took 1465.114081 s
 
2022-05-12 00:15:06.032823: 
epoch:  6 
2022-05-12 00:38:32.397578: train loss : 1.0157 
2022-05-12 00:39:29.117908: validation loss: 1.0114 
2022-05-12 00:39:29.118565: Average global foreground Dice: [0.0] 
2022-05-12 00:39:29.118717: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 00:39:30.053208: lr: 0.008847 
2022-05-12 00:39:30.053652: This epoch took 1464.020756 s
 
2022-05-12 00:39:30.053760: 
epoch:  7 
2022-05-12 01:03:40.463671: train loss : 1.0133 
2022-05-12 01:04:38.034274: validation loss: 1.0101 
2022-05-12 01:04:38.035067: Average global foreground Dice: [0.0] 
2022-05-12 01:04:38.035260: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 01:04:38.919161: lr: 0.008681 
2022-05-12 01:04:38.919595: This epoch took 1508.865763 s
 
2022-05-12 01:04:38.919697: 
epoch:  8 
2022-05-12 01:28:56.117968: train loss : 1.0128 
2022-05-12 01:29:53.710133: validation loss: 1.0092 
2022-05-12 01:29:53.710833: Average global foreground Dice: [0.0] 
2022-05-12 01:29:53.711020: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 01:29:54.599288: lr: 0.008514 
2022-05-12 01:29:54.599680: This epoch took 1515.679911 s
 
2022-05-12 01:29:54.599794: 
epoch:  9 
2022-05-12 01:54:48.026965: train loss : 1.0117 
2022-05-12 01:55:45.099442: validation loss: 1.0081 
2022-05-12 01:55:45.100249: Average global foreground Dice: [0.0] 
2022-05-12 01:55:45.100411: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 01:55:46.023385: lr: 0.008348 
2022-05-12 01:55:46.023822: This epoch took 1551.423953 s
 
2022-05-12 01:55:46.023928: 
epoch:  10 
2022-05-12 02:19:40.280392: train loss : 1.0116 
2022-05-12 02:20:37.535027: validation loss: 1.0074 
2022-05-12 02:20:37.535713: Average global foreground Dice: [0.0] 
2022-05-12 02:20:37.535883: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 02:20:38.419019: lr: 0.008181 
2022-05-12 02:20:38.419389: This epoch took 1492.395390 s
 
2022-05-12 02:20:38.419487: 
epoch:  11 
2022-05-12 02:43:39.420250: train loss : 1.0102 
2022-05-12 02:44:35.188342: validation loss: 1.0069 
2022-05-12 02:44:35.188976: Average global foreground Dice: [0.0] 
2022-05-12 02:44:35.189123: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 02:44:36.071637: lr: 0.008013 
2022-05-12 02:44:36.072084: This epoch took 1437.652522 s
 
2022-05-12 02:44:36.072222: 
epoch:  12 
2022-05-12 03:08:10.324779: train loss : 1.0097 
2022-05-12 03:09:09.463749: validation loss: 1.0063 
2022-05-12 03:09:09.464648: Average global foreground Dice: [0.0] 
2022-05-12 03:09:09.464900: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 03:09:10.381854: lr: 0.007845 
2022-05-12 03:09:10.382262: This epoch took 1474.309960 s
 
2022-05-12 03:09:10.382381: 
epoch:  13 
2022-05-12 03:33:09.888184: train loss : 1.0101 
2022-05-12 03:34:08.980795: validation loss: 1.0055 
2022-05-12 03:34:08.981603: Average global foreground Dice: [0.0] 
2022-05-12 03:34:08.981779: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 03:34:09.933657: lr: 0.007677 
2022-05-12 03:34:09.934080: This epoch took 1499.551624 s
 
2022-05-12 03:34:09.934187: 
epoch:  14 
2022-05-12 03:57:48.273942: train loss : 1.0093 
2022-05-12 03:58:48.518696: validation loss: 1.0052 
2022-05-12 03:58:48.519359: Average global foreground Dice: [0.0] 
2022-05-12 03:58:48.519544: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 03:58:49.435970: lr: 0.007508 
2022-05-12 03:58:49.436337: This epoch took 1479.502076 s
 
2022-05-12 03:58:49.436442: 
epoch:  15 
2022-05-12 04:23:21.888592: train loss : 1.0089 
2022-05-12 04:24:22.002823: validation loss: 1.0043 
2022-05-12 04:24:22.003468: Average global foreground Dice: [0.0] 
2022-05-12 04:24:22.003616: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 04:24:22.919971: lr: 0.007339 
2022-05-12 04:24:22.920324: This epoch took 1533.483789 s
 
2022-05-12 04:24:22.920413: 
epoch:  16 
2022-05-12 04:47:23.307055: train loss : 1.0087 
2022-05-12 04:48:18.502854: validation loss: 1.0037 
2022-05-12 04:48:18.503466: Average global foreground Dice: [0.0001] 
2022-05-12 04:48:18.503638: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 04:48:19.389010: lr: 0.007169 
2022-05-12 04:48:19.389379: This epoch took 1436.468906 s
 
2022-05-12 04:48:19.389492: 
epoch:  17 
2022-05-12 05:11:44.680207: train loss : 1.0065 
2022-05-12 05:12:41.374696: validation loss: 1.0031 
2022-05-12 05:12:41.375442: Average global foreground Dice: [0.0001] 
2022-05-12 05:12:41.375612: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 05:12:42.280530: lr: 0.006999 
2022-05-12 05:12:42.280962: This epoch took 1462.891410 s
 
2022-05-12 05:12:42.281067: 
epoch:  18 
2022-05-12 05:37:31.103092: train loss : 1.0064 
2022-05-12 05:38:26.320761: validation loss: 1.0018 
2022-05-12 05:38:26.321374: Average global foreground Dice: [0.0002] 
2022-05-12 05:38:26.321554: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 05:38:27.217309: lr: 0.006829 
2022-05-12 05:38:27.217719: This epoch took 1544.936577 s
 
2022-05-12 05:38:27.217813: 
epoch:  19 
2022-05-12 06:01:24.098245: train loss : 1.0048 
2022-05-12 06:02:20.099351: validation loss: 1.0018 
2022-05-12 06:02:20.100004: Average global foreground Dice: [0.0006] 
2022-05-12 06:02:20.100186: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 06:02:21.004131: lr: 0.006658 
2022-05-12 06:02:21.004532: This epoch took 1433.786660 s
 
2022-05-12 06:02:21.004623: 
epoch:  20 
2022-05-12 06:25:07.763661: train loss : 1.0049 
2022-05-12 06:26:02.398209: validation loss: 1.0005 
2022-05-12 06:26:02.398769: Average global foreground Dice: [0.0014] 
2022-05-12 06:26:02.398904: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 06:26:03.312964: lr: 0.006486 
2022-05-12 06:26:03.313263: This epoch took 1422.308577 s
 
2022-05-12 06:26:03.313339: 
epoch:  21 
2022-05-12 06:49:28.975130: train loss : 1.0034 
2022-05-12 06:50:26.157239: validation loss: 0.9999 
2022-05-12 06:50:26.158070: Average global foreground Dice: [0.0027] 
2022-05-12 06:50:26.158247: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 06:50:27.108357: lr: 0.006314 
2022-05-12 06:50:27.108765: This epoch took 1463.795365 s
 
2022-05-12 06:50:27.108874: 
epoch:  22 
2022-05-12 07:14:29.192948: train loss : 1.0022 
2022-05-12 07:15:26.708627: validation loss: 0.9956 
2022-05-12 07:15:26.709319: Average global foreground Dice: [0.0028] 
2022-05-12 07:15:26.709506: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 07:15:27.660903: lr: 0.006142 
2022-05-12 07:15:27.661310: This epoch took 1500.552366 s
 
2022-05-12 07:15:27.661467: 
epoch:  23 
2022-05-12 07:40:28.488181: train loss : 1.0008 
2022-05-12 07:41:23.670793: validation loss: 0.9966 
2022-05-12 07:41:23.671402: Average global foreground Dice: [0.0063] 
2022-05-12 07:41:23.671539: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 07:41:24.640565: lr: 0.005969 
2022-05-12 07:41:24.640928: This epoch took 1556.979381 s
 
2022-05-12 07:41:24.641020: 
epoch:  24 
2022-05-12 08:06:29.256668: train loss : 0.9991 
2022-05-12 08:07:24.591833: validation loss: 0.9945 
2022-05-12 08:07:24.592451: Average global foreground Dice: [0.0112] 
2022-05-12 08:07:24.592602: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 08:07:25.538460: lr: 0.005795 
2022-05-12 08:07:25.538829: This epoch took 1560.897747 s
 
2022-05-12 08:07:25.538919: 
epoch:  25 
2022-05-12 08:31:12.618220: train loss : 0.9971 
2022-05-12 08:32:08.837708: validation loss: 0.9892 
2022-05-12 08:32:08.838368: Average global foreground Dice: [0.0158] 
2022-05-12 08:32:08.838534: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 08:32:09.799457: lr: 0.005621 
2022-05-12 08:32:09.940209: saving checkpoint... 
2022-05-12 08:32:10.339351: done, saving took 0.54 seconds 
2022-05-12 08:32:10.506173: This epoch took 1484.967188 s
 
2022-05-12 08:32:10.506429: 
epoch:  26 
2022-05-12 08:55:27.262631: train loss : 0.9967 
2022-05-12 08:56:23.427627: validation loss: 0.9889 
2022-05-12 08:56:23.428268: Average global foreground Dice: [0.0254] 
2022-05-12 08:56:23.428417: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 08:56:24.378751: lr: 0.005446 
2022-05-12 08:56:24.411033: saving checkpoint... 
2022-05-12 08:56:24.668345: done, saving took 0.29 seconds 
2022-05-12 08:56:24.674390: This epoch took 1454.167372 s
 
2022-05-12 08:56:24.674806: 
epoch:  27 
2022-05-12 09:19:02.332343: train loss : 0.9926 
2022-05-12 09:19:59.926194: validation loss: 0.9862 
2022-05-12 09:19:59.926890: Average global foreground Dice: [0.0332] 
2022-05-12 09:19:59.927065: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 09:20:00.892537: lr: 0.005271 
2022-05-12 09:20:00.932121: saving checkpoint... 
2022-05-12 09:20:01.199268: done, saving took 0.31 seconds 
2022-05-12 09:20:01.204949: This epoch took 1416.529998 s
 
2022-05-12 09:20:01.205207: 
epoch:  28 
2022-05-12 09:43:43.476089: train loss : 0.9875 
2022-05-12 09:44:40.623141: validation loss: 0.9874 
2022-05-12 09:44:40.623760: Average global foreground Dice: [0.0512] 
2022-05-12 09:44:40.623916: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 09:44:41.584634: lr: 0.005095 
2022-05-12 09:44:41.622438: saving checkpoint... 
2022-05-12 09:44:41.885604: done, saving took 0.30 seconds 
2022-05-12 09:44:41.891418: This epoch took 1480.686105 s
 
2022-05-12 09:44:41.891691: 
epoch:  29 
2022-05-12 10:09:48.451568: train loss : 0.9836 
2022-05-12 10:10:44.640162: validation loss: 0.9775 
2022-05-12 10:10:44.640841: Average global foreground Dice: [0.0715] 
2022-05-12 10:10:44.641018: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 10:10:45.607620: lr: 0.004918 
2022-05-12 10:10:45.638983: saving checkpoint... 
2022-05-12 10:10:45.911568: done, saving took 0.30 seconds 
2022-05-12 10:10:45.915953: This epoch took 1564.024161 s
 
2022-05-12 10:10:45.916201: 
epoch:  30 
