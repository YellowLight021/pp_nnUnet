Starting... 
2022-05-12 11:08:51.831408: Using splits from existing split file: /home/aistudio/Dataset/nnUnet_preprocessed/Task006_Lung/splits_final.pkl 
2022-05-12 11:08:51.832147: The split file contains 5 splits. 
2022-05-12 11:08:51.832267: Desired fold for training: 4 
2022-05-12 11:08:51.832317: This split has 51 training and 12 validation cases. 
2022-05-12 11:08:52.012328: TRAINING KEYS:
 odict_keys(['lung_001', 'lung_004', 'lung_005', 'lung_006', 'lung_009', 'lung_010', 'lung_014', 'lung_015', 'lung_016', 'lung_018', 'lung_020', 'lung_022', 'lung_023', 'lung_026', 'lung_027', 'lung_028', 'lung_029', 'lung_031', 'lung_033', 'lung_034', 'lung_036', 'lung_037', 'lung_038', 'lung_041', 'lung_042', 'lung_043', 'lung_044', 'lung_046', 'lung_047', 'lung_048', 'lung_049', 'lung_053', 'lung_057', 'lung_058', 'lung_059', 'lung_062', 'lung_064', 'lung_065', 'lung_066', 'lung_069', 'lung_070', 'lung_071', 'lung_074', 'lung_075', 'lung_078', 'lung_079', 'lung_080', 'lung_081', 'lung_083', 'lung_084', 'lung_086']) 
2022-05-12 11:08:52.012588: VALIDATION KEYS:
 odict_keys(['lung_003', 'lung_025', 'lung_045', 'lung_051', 'lung_054', 'lung_055', 'lung_061', 'lung_073', 'lung_092', 'lung_093', 'lung_095', 'lung_096']) 
2022-05-12 11:08:56.833448: loading checkpoint /home/aistudio/Dataset/nnUnet_trained_models/nnUNet/3d_lowres/Task006_Lung/nnUNetTrainerV2__nnUNetPlansv2.1/fold_4/model_best.model train= True 
2022-05-12 11:08:57.335340: lr: 0.004918 
2022-05-12 11:08:59.956730: Unable to plot network architecture: 
2022-05-12 11:08:59.956991: No module named 'hiddenlayer' 
2022-05-12 11:08:59.957067: 
printing the network instead:
 
2022-05-12 11:08:59.957112: Generic_UNet(
  (conv_blocks_localization): LayerList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(640, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(512, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(256, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(256, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(128, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(128, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(64, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(64, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(32, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (conv_blocks_context): LayerList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(1, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(32, 32, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=32, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(64, 64, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=64, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(128, 128, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=128, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(128, 256, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(256, 256, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=256, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3D(256, 320, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
          (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
          (lrelu): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], stride=[1, 2, 2], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3D(320, 320, kernel_size=[3, 3, 3], padding=[1, 1, 1], data_format=NCDHW)
            (instnorm): InstanceNorm3D(num_features=320, epsilon=1e-05)
            (lrelu): LeakyReLU(negative_slope=0.01)
          )
        )
      )
    )
  )
  (td): LayerList()
  (tu): LayerList(
    (0): Conv3DTranspose(320, 320, kernel_size=[1, 2, 2], stride=[1, 2, 2], data_format=NCDHW)
    (1): Conv3DTranspose(320, 256, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (2): Conv3DTranspose(256, 128, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (3): Conv3DTranspose(128, 64, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
    (4): Conv3DTranspose(64, 32, kernel_size=[2, 2, 2], stride=[2, 2, 2], data_format=NCDHW)
  )
  (seg_outputs): LayerList(
    (0): Conv3D(320, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (1): Conv3D(256, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (2): Conv3D(128, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (3): Conv3D(64, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
    (4): Conv3D(32, 2, kernel_size=[1, 1, 1], data_format=NCDHW)
  )
) 
2022-05-12 11:08:59.959678: 
 
2022-05-12 11:08:59.959834: 
epoch:  30 
2022-05-12 11:31:39.798936: train loss : 0.9735 
2022-05-12 11:32:38.179880: validation loss: 0.9625 
2022-05-12 11:32:38.180404: Average global foreground Dice: [0.1554] 
2022-05-12 11:32:38.180530: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 11:32:39.159800: lr: 0.004741 
2022-05-12 11:32:39.282494: saving checkpoint... 
2022-05-12 11:32:39.677992: done, saving took 0.52 seconds 
2022-05-12 11:32:39.681439: This epoch took 1419.721535 s
 
2022-05-12 11:32:39.681616: 
epoch:  31 
2022-05-12 11:55:22.727225: train loss : 0.9612 
2022-05-12 11:56:19.919714: validation loss: 0.9574 
2022-05-12 11:56:19.920319: Average global foreground Dice: [0.2231] 
2022-05-12 11:56:19.920457: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 11:56:20.888157: lr: 0.004563 
2022-05-12 11:56:20.922621: saving checkpoint... 
2022-05-12 11:56:21.321870: done, saving took 0.43 seconds 
2022-05-12 11:56:21.325376: This epoch took 1421.643696 s
 
2022-05-12 11:56:21.325558: 
epoch:  32 
2022-05-12 12:19:21.070191: train loss : 0.9399 
2022-05-12 12:20:18.578677: validation loss: 0.9236 
2022-05-12 12:20:18.579266: Average global foreground Dice: [0.3169] 
2022-05-12 12:20:18.579396: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 12:20:19.558709: lr: 0.004384 
2022-05-12 12:20:19.593611: saving checkpoint... 
2022-05-12 12:20:19.991092: done, saving took 0.43 seconds 
2022-05-12 12:20:19.994812: This epoch took 1438.669189 s
 
2022-05-12 12:20:19.995022: 
epoch:  33 
2022-05-12 12:43:03.869992: train loss : 0.9073 
2022-05-12 12:44:01.017017: validation loss: 0.9226 
2022-05-12 12:44:01.017595: Average global foreground Dice: [0.1515] 
2022-05-12 12:44:01.017731: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 12:44:02.005922: lr: 0.004204 
2022-05-12 12:44:02.006256: This epoch took 1422.011168 s
 
2022-05-12 12:44:02.006346: 
epoch:  34 
2022-05-12 13:06:29.854214: train loss : 0.8788 
2022-05-12 13:07:28.004303: validation loss: 0.8907 
2022-05-12 13:07:28.004919: Average global foreground Dice: [0.202] 
2022-05-12 13:07:28.005068: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 13:07:29.004862: lr: 0.004023 
2022-05-12 13:07:29.038737: saving checkpoint... 
2022-05-12 13:07:29.458434: done, saving took 0.45 seconds 
2022-05-12 13:07:29.462163: This epoch took 1407.455756 s
 
2022-05-12 13:07:29.462334: 
epoch:  35 
2022-05-12 13:29:51.167798: train loss : 0.8522 
2022-05-12 13:30:48.167801: validation loss: 0.8619 
2022-05-12 13:30:48.168356: Average global foreground Dice: [0.3799] 
2022-05-12 13:30:48.168490: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 13:30:49.204220: lr: 0.003842 
2022-05-12 13:30:49.237703: saving checkpoint... 
2022-05-12 13:30:49.636974: done, saving took 0.43 seconds 
2022-05-12 13:30:49.640564: This epoch took 1400.178164 s
 
2022-05-12 13:30:49.640707: 
epoch:  36 
2022-05-12 13:53:02.364040: train loss : 0.8070 
2022-05-12 13:53:59.341993: validation loss: 0.7922 
2022-05-12 13:53:59.342581: Average global foreground Dice: [0.357] 
2022-05-12 13:53:59.342761: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 13:54:00.356627: lr: 0.003659 
2022-05-12 13:54:00.390961: saving checkpoint... 
2022-05-12 13:54:00.793869: done, saving took 0.44 seconds 
2022-05-12 13:54:00.797709: This epoch took 1391.156935 s
 
2022-05-12 13:54:00.797899: 
epoch:  37 
2022-05-12 14:17:18.241422: train loss : 0.7913 
2022-05-12 14:18:15.403810: validation loss: 0.7862 
2022-05-12 14:18:15.404381: Average global foreground Dice: [0.4006] 
2022-05-12 14:18:15.404524: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 14:18:16.439022: lr: 0.003476 
2022-05-12 14:18:16.472750: saving checkpoint... 
2022-05-12 14:18:16.898149: done, saving took 0.46 seconds 
2022-05-12 14:18:16.900995: This epoch took 1456.103024 s
 
2022-05-12 14:18:16.901151: 
epoch:  38 
2022-05-12 14:41:09.031194: train loss : 0.7642 
2022-05-12 14:42:07.041293: validation loss: 0.7468 
2022-05-12 14:42:07.041881: Average global foreground Dice: [0.49] 
2022-05-12 14:42:07.042028: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 14:42:08.032131: lr: 0.003291 
2022-05-12 14:42:08.067269: saving checkpoint... 
2022-05-12 14:42:08.524953: done, saving took 0.49 seconds 
2022-05-12 14:42:08.528503: This epoch took 1431.627286 s
 
2022-05-12 14:42:08.528683: 
epoch:  39 
2022-05-12 15:05:58.404633: train loss : 0.7561 
2022-05-12 15:06:55.938327: validation loss: 0.7040 
2022-05-12 15:06:55.938872: Average global foreground Dice: [0.4937] 
2022-05-12 15:06:55.938998: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 15:06:56.933097: lr: 0.003106 
2022-05-12 15:06:56.966886: saving checkpoint... 
2022-05-12 15:06:57.413410: done, saving took 0.48 seconds 
2022-05-12 15:06:57.416227: This epoch took 1488.887477 s
 
2022-05-12 15:06:57.416403: 
epoch:  40 
2022-05-12 15:29:50.285071: train loss : 0.7161 
2022-05-12 15:30:47.713886: validation loss: 0.7033 
2022-05-12 15:30:47.714424: Average global foreground Dice: [0.5899] 
2022-05-12 15:30:47.714546: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 15:30:48.703552: lr: 0.002919 
2022-05-12 15:30:48.737190: saving checkpoint... 
2022-05-12 15:30:49.175942: done, saving took 0.47 seconds 
2022-05-12 15:30:49.180118: This epoch took 1431.763642 s
 
2022-05-12 15:30:49.180314: 
epoch:  41 
2022-05-12 15:52:55.375135: train loss : 0.6943 
2022-05-12 15:53:56.500687: validation loss: 0.7547 
2022-05-12 15:53:56.501430: Average global foreground Dice: [0.3072] 
2022-05-12 15:53:56.501603: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 15:53:57.718249: lr: 0.00273 
2022-05-12 15:53:57.718615: This epoch took 1388.538230 s
 
2022-05-12 15:53:57.718747: 
epoch:  42 
2022-05-12 16:17:41.883071: train loss : 0.6842 
2022-05-12 16:18:40.233037: validation loss: 0.6701 
2022-05-12 16:18:40.233634: Average global foreground Dice: [0.5699] 
2022-05-12 16:18:40.233773: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 16:18:41.239361: lr: 0.002541 
2022-05-12 16:18:41.275194: saving checkpoint... 
2022-05-12 16:18:41.805445: done, saving took 0.57 seconds 
2022-05-12 16:18:41.809370: This epoch took 1484.090307 s
 
2022-05-12 16:18:41.809541: 
epoch:  43 
2022-05-12 16:42:30.668880: train loss : 0.6854 
2022-05-12 16:43:29.228016: validation loss: 0.6658 
2022-05-12 16:43:29.228559: Average global foreground Dice: [0.5694] 
2022-05-12 16:43:29.228687: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 16:43:30.232464: lr: 0.002349 
2022-05-12 16:43:30.268329: saving checkpoint... 
2022-05-12 16:43:30.740163: done, saving took 0.51 seconds 
2022-05-12 16:43:30.744581: This epoch took 1488.934969 s
 
2022-05-12 16:43:30.744801: 
epoch:  44 
2022-05-12 17:06:31.546893: train loss : 0.6422 
2022-05-12 17:07:30.507953: validation loss: 0.6337 
2022-05-12 17:07:30.508550: Average global foreground Dice: [0.5214] 
2022-05-12 17:07:30.508685: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 17:07:31.513972: lr: 0.002156 
2022-05-12 17:07:31.550031: saving checkpoint... 
2022-05-12 17:07:31.999836: done, saving took 0.49 seconds 
2022-05-12 17:07:32.004030: This epoch took 1441.259133 s
 
2022-05-12 17:07:32.004217: 
epoch:  45 
2022-05-12 17:31:45.807367: train loss : 0.6477 
2022-05-12 17:32:45.147609: validation loss: 0.6302 
2022-05-12 17:32:45.148253: Average global foreground Dice: [0.5584] 
2022-05-12 17:32:45.148396: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 17:32:46.156380: lr: 0.001961 
2022-05-12 17:32:46.192373: saving checkpoint... 
2022-05-12 17:32:46.648997: done, saving took 0.49 seconds 
2022-05-12 17:32:46.653370: This epoch took 1514.649072 s
 
2022-05-12 17:32:46.653569: 
epoch:  46 
2022-05-12 17:55:32.920977: train loss : 0.6386 
2022-05-12 17:56:30.751841: validation loss: 0.6506 
2022-05-12 17:56:30.752497: Average global foreground Dice: [0.51] 
2022-05-12 17:56:30.752686: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 17:56:31.792465: lr: 0.001764 
2022-05-12 17:56:31.828882: saving checkpoint... 
2022-05-12 17:56:32.081693: done, saving took 0.29 seconds 
2022-05-12 17:56:32.085912: This epoch took 1425.432274 s
 
2022-05-12 17:56:32.086103: 
epoch:  47 
2022-05-12 18:20:32.632020: train loss : 0.6114 
2022-05-12 18:21:28.610362: validation loss: 0.6171 
2022-05-12 18:21:28.610973: Average global foreground Dice: [0.6159] 
2022-05-12 18:21:28.611123: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 18:21:29.595486: lr: 0.001564 
2022-05-12 18:21:29.631516: saving checkpoint... 
2022-05-12 18:21:29.876374: done, saving took 0.28 seconds 
2022-05-12 18:21:29.880584: This epoch took 1497.794407 s
 
2022-05-12 18:21:29.880754: 
epoch:  48 
2022-05-12 18:46:03.791370: train loss : 0.6133 
2022-05-12 18:46:58.408324: validation loss: 0.6688 
2022-05-12 18:46:58.408927: Average global foreground Dice: [0.5057] 
2022-05-12 18:46:58.409073: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 18:46:59.387999: lr: 0.001361 
2022-05-12 18:46:59.422836: saving checkpoint... 
2022-05-12 18:46:59.673642: done, saving took 0.29 seconds 
2022-05-12 18:46:59.677171: This epoch took 1529.796353 s
 
2022-05-12 18:46:59.677701: 
epoch:  49 
2022-05-12 19:09:37.500174: train loss : 0.6131 
2022-05-12 19:10:32.710101: validation loss: 0.6033 
2022-05-12 19:10:32.710685: Average global foreground Dice: [0.6364] 
2022-05-12 19:10:32.710812: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 19:10:33.701779: lr: 0.001155 
2022-05-12 19:10:33.702079: saving scheduled checkpoint file... 
2022-05-12 19:10:33.737724: saving checkpoint... 
2022-05-12 19:10:33.997855: done, saving took 0.30 seconds 
2022-05-12 19:10:34.000853: done 
2022-05-12 19:10:34.036129: saving checkpoint... 
2022-05-12 19:10:34.382252: done, saving took 0.38 seconds 
2022-05-12 19:10:34.385313: This epoch took 1414.707539 s
 
2022-05-12 19:10:34.385504: 
epoch:  50 
2022-05-12 19:33:07.179897: train loss : 0.5976 
2022-05-12 19:34:02.087098: validation loss: 0.5687 
2022-05-12 19:34:02.087678: Average global foreground Dice: [0.6312] 
2022-05-12 19:34:02.087825: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 19:34:03.072536: lr: 0.000945 
2022-05-12 19:34:03.108176: saving checkpoint... 
2022-05-12 19:34:03.449973: done, saving took 0.38 seconds 
2022-05-12 19:34:03.453590: This epoch took 1409.068007 s
 
2022-05-12 19:34:03.453761: 
epoch:  51 
2022-05-12 19:58:09.241025: train loss : 0.5978 
2022-05-12 19:59:03.646713: validation loss: 0.6118 
2022-05-12 19:59:03.647307: Average global foreground Dice: [0.583] 
2022-05-12 19:59:03.647450: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 19:59:04.630781: lr: 0.00073 
2022-05-12 19:59:04.665431: saving checkpoint... 
2022-05-12 19:59:05.008706: done, saving took 0.38 seconds 
2022-05-12 19:59:05.012845: This epoch took 1501.558997 s
 
2022-05-12 19:59:05.013012: 
epoch:  52 
2022-05-12 20:22:24.347662: train loss : 0.5885 
2022-05-12 20:23:20.240564: validation loss: 0.5725 
2022-05-12 20:23:20.241171: Average global foreground Dice: [0.6316] 
2022-05-12 20:23:20.241302: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 20:23:21.225468: lr: 0.000507 
2022-05-12 20:23:21.259279: saving checkpoint... 
2022-05-12 20:23:21.612156: done, saving took 0.39 seconds 
2022-05-12 20:23:21.615144: This epoch took 1456.602061 s
 
2022-05-12 20:23:21.615341: 
epoch:  53 
2022-05-12 20:48:01.865092: train loss : 0.5954 
2022-05-12 20:48:58.345369: validation loss: 0.6603 
2022-05-12 20:48:58.345957: Average global foreground Dice: [0.4768] 
2022-05-12 20:48:58.346105: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 20:48:59.356462: lr: 0.000271 
2022-05-12 20:48:59.356797: This epoch took 1537.741366 s
 
2022-05-12 20:48:59.356908: 
epoch:  54 
2022-05-12 21:13:17.017539: train loss : 0.5934 
2022-05-12 21:14:13.398416: validation loss: 0.5875 
2022-05-12 21:14:13.398979: Average global foreground Dice: [0.6344] 
2022-05-12 21:14:13.399109: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2022-05-12 21:14:14.420905: lr: 0.0 
2022-05-12 21:14:14.488440: saving checkpoint... 
2022-05-12 21:14:15.040090: done, saving took 0.62 seconds 
2022-05-12 21:14:15.044110: This epoch took 1515.687127 s
 
2022-05-12 21:14:15.081677: saving checkpoint... 
2022-05-12 21:14:15.486794: done, saving took 0.44 seconds 
2022-05-12 21:22:36.636014: finished prediction 
2022-05-12 21:22:36.636487: evaluation of raw predictions 
2022-05-12 21:22:50.248535: determining postprocessing 
